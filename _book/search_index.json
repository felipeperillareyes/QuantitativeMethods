[["index.html", "Intermediate Quantitative Methods About 0.1 How to use these exercises? 0.2 Schedule", " Intermediate Quantitative Methods Lucas Lemmann 2023-12-11 About What is this book about? What for? 0.1 How to use these exercises? Besides the 14 lectures, the course will be organized around 12 non-graded exercises: 5 labs 7 do-it-yourself (DIYS) The labs’ solutions will be discussed in detail between TAs and students in the corresponding sessions, while DIYS will not. In both cases, we will publish the solutions the week after the exercise is due. We encourage you to prepare for the lab sessions in advance as well as to attend them to discuss any doubts they might have related to the labs material. To prevent redundant communications (i.e., emails with the same information), share your questions regarding the exercises in the forum. Labs will emphasize the most voted questions. While we encourage and foster a collaborative learning process, we expect you to work individually first. I.e., try to address the task on your own first, identify what is limiting you, try to solve it on your own (not for too long), and, if you cannot find a solution, reach out your classmates. Once you find your solution, consider discussing the solution with your classmates. All the files behind this book can be found in its GitHub repository. 0.2 Schedule Week Dates Exercise type 1 19-25/02 DIYS 1 2 26/02-03/03 Lab 1 3 04/03-10/03 Lab 1 4 11/03-17/03 DIYS 2 5 18/03-24/03 Lab 2 6 25/03-31/03 DIYS 3 Spring Break 28/03-07/04 None? 7 08/04-14/04 Lab 3 8 15/04-21/04 DIYS 4 9 22/04-28/04 Lab 4 10 29/04-05/05 DIYS 5 11 06/05-12/05 Lab 5 12 13/05-19/05 Lab 5 13 20/05-26/05 DIYS 6 14 27/05-02/06 DIYS 7 "],["week-1-diys-1.html", "Chapter 1 Week 1: DIYS 1 1.1 Aims: 1.2 First Part: REPETITION R/RStudio 1.3 Second Part: Some Simple Analysis", " Chapter 1 Week 1: DIYS 1 1.1 Aims: To refresh your R skills. Performing some basic analyses (i.e., descriptive, exploratory, and hypothesis testing ones). 1.2 First Part: REPETITION R/RStudio Download the files f.txt and m.txt. They contain information on screentime (i.e., number of seconds per day spent on mobile phone), and closeness to a populist party for female and male individuals respectively. Open them and explore the first 3 observations for each file. Note: remember that this book was developed as an R-project, which is a way to ensure reproducibility of projects. One of the advantages of R-projects is that files are stored in directories (i.e., folders) that are defined relative to the R-project. For example, if the folder Data is stored in the same folder as the project, files stored in Data can be accessed as follows: female &lt;- read.table(\"Data/f.txt\", header = TRUE, sep = \"\\t\").To learn more about R-projects, check this entry. # Your code goes here For the exercise before publishing the solution # open data female &lt;- read.table(&quot;Data/f.txt&quot;, header = TRUE, sep = &quot;;&quot;) # explore data head(female, 3) ## ID ScreenTime ClosenessPopulistParty ## 1 3 15000 1.176471 ## 2 4 14861 1.294118 ## 3 5 14861 1.294118 # open data male &lt;- read.table(&quot;Data/m.txt&quot;, header = TRUE, sep = &quot;;&quot;) # explore data head(male, 3) ## ID ScreenTime ClosenessPopulistParty ## 1 1 15000 0.7361963 ## 2 2 15000 0.7361963 ## 3 6 14861 0.6748466 Some key functions in dplyr can be categorized as dealing with columns (e.g., select, mutate), rows (e.g., filter, distinct, arrange), or groups (e.g., group_by, summarise, and count). Let’s use some of them! Select only the columns ‘ScreenTime’ and ‘ClosenessPopulistParty’. Do it only for the first three observations of the data on females. head(female, 3) %&gt;% select(ScreenTime, ClosenessPopulistParty) ## ScreenTime ClosenessPopulistParty ## 1 15000 1.176471 ## 2 14861 1.294118 ## 3 14861 1.294118 Select all columns except ‘ID’. Do not use ScreenTime nor ClosenessPopulistParty. Do it only for the first three observations of the data on females. Is the resulting table the same as the previous point? If not, check your answer. head(female, 3) %&gt;% select(-ID) ## ScreenTime ClosenessPopulistParty ## 1 15000 1.176471 ## 2 14861 1.294118 ## 3 14861 1.294118 Note: to check the documentation of select, use ?select on the console. Use mutate to create a new column in the DataFrame female called ScreenTimeTimesClosenessPopulistParty formed as the product of ScreenTime and ClosenessPopulistParty. Show the first three observations for the new variable. female &lt;-female %&gt;% mutate(ScreenTimeTimesClosenessPopulistParty= ScreenTime * ClosenessPopulistParty) head(female$ScreenTimeTimesClosenessPopulistParty,3) ## [1] 17647.06 19231.88 19231.88 Get rid of the column ScreenTimeTimesClosenessPopulistParty. Use subset. Note: remember to check that the outcome of your code is in accordance with what you expect. To do so, inspect the outcome using head. female &lt;- subset(female, select= - ScreenTimeTimesClosenessPopulistParty) head(female, 3) ## ID ScreenTime ClosenessPopulistParty ## 1 3 15000 1.176471 ## 2 4 14861 1.294118 ## 3 5 14861 1.294118 Use filter to find the share of female individuals with a ClosenessPopulistParty higher than 20 and lower than 21. f20_21 &lt;- female %&gt;% filter(ClosenessPopulistParty&gt;20, ClosenessPopulistParty&lt;21) The share of female individuals with a ClosenessPopulistParty higher than 20 and lower than 21 is: 0%. Use filter to find the share of female individuals with a ClosenessPopulistParty higher than 20 and lower than 21 while at the same time having less than 14000 ScreenTime. fClosenessPopulistParty20_21_ScreenTime14000 &lt;- female %&gt;% filter(ClosenessPopulistParty&gt;20, ClosenessPopulistParty&lt;21, ScreenTime&lt;14000) The share of female individuals with a ClosenessPopulistParty higher than 20 and lower than 21 while at the same time having less than 14000 is:“, nrow(fClosenessPopulistParty20_21_ScreenTime14000)*100/nrow(female) Use filter to find the share of male individuals with ID number lower than 5 and higher than 860. Notice that you can use either &amp; between conditions or simply a comma. Could any dataset generate a different answer? Why? m_5_860 &lt;- male %&gt;% filter(ID&lt;5 &amp; ID&gt;860) The share of male individuals with ID number lower than 5 AND higher than 860 is: 0 Use filter to find the share of male individuals with ID number lower than 5 or higher than 860. Use | between conditions. Could any dataset generate a different answer? Why? m_5_or_860 &lt;- male %&gt;% filter(ID&lt;5 | ID&gt;860) The share of male individuals with ID number lower than 5 OR higher than 860 is: 46.36. Sometimes, data is less than optimal and present repeated observations. Do you have repeated individual observations? Use distinct to identify the share of male IDs that are unique. unique_m_IDs &lt;- male %&gt;% distinct(ID) The share of male IDs that are unique is: 100 As you will notice the more you code, there are many possible ways to perform the same task. For example, an alternative to the previous solution is as follows. # Check for repeated IDs in the female dataset. How many are there? repeated_ids_female &lt;- female %&gt;% group_by(ID) %&gt;% # groups by values of ID filter(n() &gt; 1) # Keeps only groups of values with more than 1 observation The number of repeated IDs in the female dataset is: 0. Similarly, you could have used count to count the number of observations sharing all values across all columns. Use it to count how many observations are repeated. Do it for the male dataset. Unique_male &lt;- male %&gt;% count() %&gt;% filter(freq&gt;1) nrow(Unique_male) ## [1] 0 Use arrange to find the three highest and lowest ClosenessPopulistParty values for males. Use slice_head. # Max top_3_m &lt;- male %&gt;% arrange(desc(ClosenessPopulistParty)) %&gt;% #Orders in a descending order the variable CPP slice_head(n = 3) # returns only the top 3 observations print(top_3_m) ## ID ScreenTime ClosenessPopulistParty ## 1 786 7894 10 ## 2 847 7593 10 ## 3 863 7431 10 # Min bottom_3_m &lt;- male %&gt;% arrange(ClosenessPopulistParty) %&gt;%#Orders in a ascending order the variable CPP. `Arrange` proceeds like that by default. slice_head(n = 3) print(bottom_3_m) ## ID ScreenTime ClosenessPopulistParty ## 1 1170 6366 0.00000000 ## 2 614 9097 0.06134969 ## 3 615 9097 0.06134969 Use summarise to describe the maximum, minimum, mean, median, standard deviation, and percentile 10 and 90. Do it for each, the female and male datasets. summary_stats_f &lt;- female %&gt;% summarise( max_value = max(ScreenTime), min_value = min(ScreenTime), mean_value = mean(ScreenTime), median_value = median(ScreenTime), sd_value = sd(ScreenTime), percentile_10 = quantile(ScreenTime, 0.10), percentile_90 = quantile(ScreenTime, 0.90) ) summary_stats_m &lt;- male %&gt;% summarise( max_value = max(ScreenTime), min_value = min(ScreenTime), mean_value = mean(ScreenTime), median_value = median(ScreenTime), sd_value = sd(ScreenTime), percentile_10 = quantile(ScreenTime, 0.10), percentile_90 = quantile(ScreenTime, 0.90) ) summary_f_m &lt;- bind_rows( &quot;Male&quot; = summary_stats_m, &quot;Female&quot; = summary_stats_f, .id = &quot;Sex&quot; ) summary_f_m ## Sex max_value min_value mean_value median_value sd_value percentile_10 ## 1 Male 15000 301 8013.908 7894 3255.103 3333 ## 2 Female 15000 0 6891.237 7130 3674.206 1366 ## percentile_90 ## 1 12431 ## 2 11667 1.3 Second Part: Some Simple Analysis Having refreshed some basics of R, you will now perform some simple analyses. 1.3.1 First, adjust the data For each dataset, create a new variable called female. Note: it is a good practice to name your variables so their interpretation is easy. For example, instead of using sex as a variable name, using female facilitates understanding that a zero represents male while a one represents female. female$female &lt;- 1 male$female &lt;- 0 Create one DataFrame with all the IDs present in both datasets. How many cases are there? Use dplyr’s join methods. in_both &lt;- inner_join(female, male, by=&quot;ID&quot;) The number of cases where an ID is in both datasets is: 0. Since inner_join keeps the observations whose ids are shared across the merged datasets, the lack of shared individuals across the datasets generates this result. Now that you know that there are no repeated individuals across the datasets, consider whether a join method is the appropriate way of unifying both datasets. Try first with full_join and then with bind_rows. Which one should you use? Why? Finally, how many individuals does the new DataFrame have? # # Without assuming that `female` was created for each DataFrame in advance, another alternative could have been. # female &lt;- read.table(&quot;Data/f.txt&quot;, header = TRUE, sep = &quot;;&quot;) # male &lt;- read.table(&quot;Data/m.txt&quot;, header = TRUE, sep = &quot;;&quot;) # # all &lt;- bind_rows(female, male, .id = &#39;sex&#39;) # # # Which assigns a number 1 for the first binded DataFrame, and 2 for the second one. Hence, we can replace the values as follows. # all$sex &lt;- ifelse(all$sex == 1, &#39;F&#39;, ifelse(all$sex == 2, &#39;M&#39;, all$sex)) # As we have already created `female` all_fulljoin &lt;- full_join(female, male, by=&quot;ID&quot;, copy=FALSE) head(all_fulljoin) ## ID ScreenTime.x ClosenessPopulistParty.x female.x ScreenTime.y ## 1 3 15000 1.176471 1 NA ## 2 4 14861 1.294118 1 NA ## 3 5 14861 1.294118 1 NA ## 4 9 14699 1.058824 1 NA ## 5 12 14560 3.176471 1 NA ## 6 14 14560 3.235294 1 NA ## ClosenessPopulistParty.y female.y ## 1 NA NA ## 2 NA NA ## 3 NA NA ## 4 NA NA ## 5 NA NA ## 6 NA NA # Assuming that `sex` was created for each DataFrame all_bindrows &lt;- bind_rows(female, male, .id = NULL) head(all_bindrows) ## ID ScreenTime ClosenessPopulistParty female ## 1...1 3 15000 1.176471 1 ## 2...2 4 14861 1.294118 1 ## 3...3 5 14861 1.294118 1 ## 4...4 9 14699 1.058824 1 ## 5...5 12 14560 3.176471 1 ## 6...6 14 14560 3.235294 1 The new DataFrame using full_join has 1786, individuals. The new DataFrame using bind_rows also has 1786, individuals. However, as showed with the preliminary inspections made using head(), bind_rows is the adequate tool as the shared variables across sources are taken as belonging to the same variable and are hence stack one on top of the other. On the contrary, full_join assumes that they are different and so distinguishes them by adding the ending “.X” to the variable names of data present in the first dataset and “.Y” to the variable names of data present in the second dataset. What’s the share per sex in the unified DataFrame from the previous point? Consider using group_by and summarize # Calculating shares shares &lt;- all_bindrows %&gt;% group_by(female) %&gt;% dplyr::summarize( count = n(), share = n() / nrow(all_bindrows) ) # View the result print(shares) ## # A tibble: 2 × 3 ## female count share ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 865 0.484 ## 2 1 921 0.516 1.3.2 Second, answer the following questions Given that your original datasets are random samples, there is uncertainty about how the average of the different variables vary across sexes. Hence, to further qualify your understanding you need to answer the following quetions: Is the average number of ScreenTime for males and females statistically different? Use a t-test. # Conduct a t-test t_test_result &lt;- t.test(ScreenTime ~ female, data = all_bindrows) # Print the result print(t_test_result) ## ## Welch Two Sample t-test ## ## data: ScreenTime by female ## t = 6.8441, df = 1778, p-value = 1.055e-11 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## 800.9513 1444.3903 ## sample estimates: ## mean in group 0 mean in group 1 ## 8013.908 6891.237 Are ClosenessPopulistParty and daily ScreenTime correlated? If so, how? # Perform Pearson&#39;s correlation test correlation_test &lt;- cor.test(all_bindrows$ClosenessPopulistParty, all_bindrows$ScreenTime, method = &quot;pearson&quot;) # Print the results print(correlation_test) ## ## Pearson&#39;s product-moment correlation ## ## data: all_bindrows$ClosenessPopulistParty and all_bindrows$ScreenTime ## t = -12.515, df = 1784, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.3261789 -0.2408848 ## sample estimates: ## cor ## -0.2840939 Does that correlation depend on whether individuals are of one sex or another? If so, how? # Correlation test for females correlation_female &lt;- all_bindrows %&gt;% filter(female == 1) %&gt;% with(cor.test(ClosenessPopulistParty, ScreenTime, method = &quot;pearson&quot;)) # Correlation test for males correlation_male &lt;- all_bindrows %&gt;% filter(female == 0) %&gt;% with(cor.test(ClosenessPopulistParty, ScreenTime, method = &quot;pearson&quot;)) Thus, the correlation for Females is , while it is for Males. Finally, graph ScreenTime vs ClosenessPopulistParty by each sex and the for both together. What does this graph teach you? Graph for females here. ggplot(female, aes(x = ScreenTime, y = ClosenessPopulistParty)) + geom_point() + labs(x = &quot;Screen Time&quot;, y = &quot;Closeness to Populist Party&quot;) + ggtitle(&quot;Scatterplot of Screen Time vs Closeness to Populist Party (females)&quot;) Graph for males here. ggplot(male, aes(x = ScreenTime, y = ClosenessPopulistParty)) + geom_point() + labs(x = &quot;Screen Time&quot;, y = &quot;Closeness to Populist Party&quot;) + ggtitle(&quot;Scatterplot of Screen Time vs Closeness to Populist Party (males)&quot;) Graph for females and males together here. ggplot(all_bindrows, aes(x = ScreenTime, y = ClosenessPopulistParty, color = as.factor(female))) + geom_point() + labs(x = &quot;Screen Time&quot;, y = &quot;Closeness to Populist Party&quot;, color = &quot;Female&quot;) + ggtitle(&quot;Scatterplot of Screen Time vs Closeness to Populist Party by Sex&quot;) + scale_color_manual(values = c(&quot;1&quot; = &quot;blue&quot;, &quot;0&quot; = &quot;red&quot;)) # Define color mapping This exercise was a cautionary tale that aims at highlighting the importance to perform preliminary exploratory analysis on your data. What are the gorillas of your data? For example, have you found people with heights of 10 meters? Have you found systematic but unexpected relationships in the data? What are they due to? Mistakes in data management or measurement? To the phenomenon itself? What relations between variables could guide your analysis? Preliminary analyses include but are not restricted to: Generate scatter plots across many variables ggpairs(all_bindrows) pairs(all_bindrows) ii. Generate histograms ggplot(all_bindrows, aes_string(x = &#39;ScreenTime&#39;)) + geom_histogram(bins = 30, fill = &quot;blue&quot;, color = &quot;black&quot;) + labs(title = paste(&quot;Histogram of&quot;, &#39;ScreenTime&#39;), x = &#39;ScreenTime&#39;, y = &quot;Frequency&quot;) ggplot(all_bindrows, aes_string(x = &quot;ClosenessPopulistParty&quot;)) + geom_histogram(bins = 30, fill = &quot;blue&quot;, color = &quot;black&quot;) + labs(title = paste(&quot;Histogram of&quot;, &#39;ClosenessPopulistParty&#39;), x = &#39;ClosenessPopulistParty&#39;, y = &quot;Frequency&quot;) ggplot(all_bindrows, aes_string(x = &#39;female&#39;)) + geom_histogram(bins = 30, fill = &quot;blue&quot;, color = &quot;black&quot;) + labs(title = paste(&quot;Histogram of&quot;, &#39;female&#39;), x = &#39;female&#39;, y = &quot;Frequency&quot;) "],["week-2.html", "Chapter 2 Week 2 2.1 Exercise 2.2 Solution", " Chapter 2 Week 2 2.1 Exercise 2nd: simulated dataset and increase the variance: how does that affects the standard error 2.2 Solution Data taken from here. Original selective attention, here. Suicide awareness campaign, here. "],["week-3-regression-i-prediction.html", "Chapter 3 Week 3: Regression I (Prediction) 3.1 Aims 3.2 Exercise: Context and Question 3.3 Exercise: solution 3.4 Appendix: Getting the data first", " Chapter 3 Week 3: Regression I (Prediction) 3.1 Aims Using regression analysis for quantitative descriptive purposes with real data Using simulation to instantiate how different properties of the data generating process alter the reliability of regression analysis 3.2 Exercise: Context and Question As developed in the following entry, a referendum took place in 2021 to assess whether Swiss citizens approved or not a bill aimed at reducing greenhouse gas emissions. The bill was rejected by a narrow margin: 51.6% of votes went for the “No”, while 48.4% for the “Yes”. In the media, some analysts commented that a marked division or cleavage between rural and urban voters seems to have led to this result. According to their narrative, a specially high rural turn-out seems to underlie such result. As the analyst commented, most urban inhabitants liked the bill while most rural inhabitants disliked it. Not satisfied with a simple impression, as a political scientist you want to quantitatively qualify your understanding about what happened in that election. Particularly, you want to know: Given that more rural municipalities have smaller populations, was the turn-out higher in municipalities with smaller populations than in bigger ones? Was the turn-out higher in more agricultural municipalities than in less agricultural ones? Thus, to analytically describe the voting results, you will perform some regressions using the following data. 3.3 Exercise: solution Note: Remember that you can copy the code from one point to answer another point. Simply make the necessary adjustments. 3.3.1 Real data Open the file CO2_land.csv. CO2_land &lt;- read.csv(&quot;Data/CO2_land.csv&quot;, header = TRUE) Note: in the Appendix you can find how to get the data using APIs of the Swiss Federal Statistical Office. Using such a file, you embark in doing the following to answer your two questions: Since you want to describe the turn-out across municipalities with different population sizes, you initially think of regressing the turn-out on the population size. As you lack the municipal population, you use the number of eligible voters anzahlStimmberechtigte as a proxy. To have a preliminary notion on the relation between stimmbeteiligungInProzent (Turn-out in percent) on anzahlStimmberechtigte (number of eligible voters), generate a scatter plot and add as many straight lines as you want to visually guess which one would be the one that goes closest to each data point. This entry could be of help. Remember that a straight line is defined in therms of its slope and intercept parameters. Begin with a slope of zero and an intercept equal to 50. Then, change only one parameter at a time. guess_int_1= 50 guess_slope_1= 0 guess_int_2= mean(CO2_land$stimmbeteiligungInProzent, na.rm = TRUE) guess_slope_3=-0.01 guess_slope_4=-0.0005 guess_slope_5=-0.0003 ggplot(CO2_land, aes(x = anzahlStimmberechtigte, y = stimmbeteiligungInProzent)) + geom_point() + geom_abline(intercept = guess_int_1, slope = guess_slope_1, color = &quot;#FF0000&quot;, size = 1) + #first guess geom_abline(intercept = guess_int_2, slope = guess_slope_1, color = &quot;#FF4500&quot;, size = 1) + #a good guess, only changing intercept geom_abline(intercept = guess_int_2, slope = guess_slope_3, color = &quot;#FFA500&quot;, size = 1) + #decreasing the slope, keeping the intercept geom_abline(intercept = guess_int_2, slope = guess_slope_4, color = &quot;#9ACD32&quot;, size = 1) + #decreasing the slope, keeping the intercept geom_abline(intercept = guess_int_2, slope = guess_slope_5, color = &quot;#008000&quot;, size = 1) + #best visual guess scale_y_continuous(limits = c(0, max(CO2_land$stimmbeteiligungInProzent, na.rm = TRUE))) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Number of eligible voters&quot;, y = &quot;Turn-out (%)&quot;, title = &quot;Scatter plot with straight lines using the eye-meter&quot; ) Add to the previous graph the linear regression line. ggplot(CO2_land, aes(x = anzahlStimmberechtigte, y = stimmbeteiligungInProzent)) + geom_point() + geom_abline(intercept = guess_int_1, slope = guess_slope_1, color = &quot;#FF0000&quot;, size = 1) + #first guess geom_abline(intercept = guess_int_2, slope = guess_slope_1, color = &quot;#FF4500&quot;, size = 1) + #a good guess, only changing intercept geom_abline(intercept = guess_int_2, slope = guess_slope_3, color = &quot;#FFA500&quot;, size = 1) + #decreasing the slope, keeping the intercept geom_abline(intercept = guess_int_2, slope = guess_slope_4, color = &quot;#9ACD32&quot;, size = 1) + #decreasing the slope, keeping the intercept geom_abline(intercept = guess_int_2, slope = guess_slope_5, color = &quot;#008000&quot;, size = 1) + #best visual guess geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;green&quot;) + #mathematically estimated best possible parameters scale_y_continuous(limits = c(0, max(CO2_land$stimmbeteiligungInProzent, na.rm = TRUE))) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Number of eligible voters&quot;, y = &quot;Turn-out (%)&quot;, title = &quot;Scatter plot with straight lines using the eye-meter and the regression line&quot; ) You have now instantiated the intuition behind the linear regression by forming different lines that are visually intuitive about the relation between our variables of interest. Since you know that the linear regression line is the straight line that is closest to all points, you want to go beyond your “eye-meter” and use a precise measurement. Regress stimmbeteiligungInProzent on anzahlStimmberechtigte. Report and interpret the parameters. Write the regression here. result1 &lt;- lm(stimmbeteiligungInProzent ~ anzahlStimmberechtigte , data = CO2_land) summary(result1) ## ## Call: ## lm(formula = stimmbeteiligungInProzent ~ anzahlStimmberechtigte, ## data = CO2_land) ## ## Residuals: ## Min 1Q Median 3Q Max ## -26.173 -5.185 0.244 5.365 45.871 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.558e+01 1.832e-01 358.048 &lt;2e-16 *** ## anzahlStimmberechtigte -2.202e-04 2.408e-05 -9.146 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.98 on 2130 degrees of freedom ## Multiple R-squared: 0.03779, Adjusted R-squared: 0.03734 ## F-statistic: 83.65 on 1 and 2130 DF, p-value: &lt; 2.2e-16 Report and interpret the parameters here: coeffs1&lt;- coef(result1) coeffs1 ## (Intercept) anzahlStimmberechtigte ## 65.5802858731 -0.0002202479 The estimated (Intercept) is 65.58, while the estimated slope for anzahlStimmberechtigte is -2.2^{-4}. It means that, across Swiss municipalities, an increase of a thousand voters was associated with an average decrease in the turn-out of -0.22 percentage points. Since you don’t want to take for granted that the mathematically estimated parameters are the ones that generate the best possible fit for a straight line, you want to compare the Sum of Squared Residuals (SSR) across the lines that you generated in the previous point. Generate and compare the SSR for each of those lines. Are the parameters estimated by R the best among your estimations? # It&#39;s easier to calculate the residuals simply using the model CO2_land$ResidRegression&lt;- residuals(result1) CO2_land$SqResidRegression&lt;- (residuals(result1))^2 CO2_land$SumSqResidRegression&lt;- sum(CO2_land$SqResidRegression) #But it&#39;s also possible to easily calculate the squared residuals (SR) for each model CO2_land$SR_Guess1= ((guess_int_1 + guess_slope_1 * CO2_land$anzahlStimmberechtigte)-CO2_land$stimmbeteiligungInProzent)^2 CO2_land$SR_Guess2= ((guess_int_2 + guess_slope_1 * CO2_land$anzahlStimmberechtigte)-CO2_land$stimmbeteiligungInProzent)^2 CO2_land$SR_Guess3= ((guess_int_1 + guess_slope_3 * CO2_land$anzahlStimmberechtigte)-CO2_land$stimmbeteiligungInProzent)^2 CO2_land$SR_Guess4= ((guess_int_1 + guess_slope_4 * CO2_land$anzahlStimmberechtigte)-CO2_land$stimmbeteiligungInProzent)^2 CO2_land$SR_Guess5= ((guess_int_1 + guess_slope_5 * CO2_land$anzahlStimmberechtigte)-CO2_land$stimmbeteiligungInProzent)^2 # And sum of squared residuals (SSR) CO2_land$SSR_Guess1= sum(CO2_land$SR_Guess1) CO2_land$SSR_Guess2= sum(CO2_land$SR_Guess2) CO2_land$SSR_Guess3= sum(CO2_land$SR_Guess3) CO2_land$SSR_Guess4= sum(CO2_land$SR_Guess4) CO2_land$SSR_Guess5= sum(CO2_land$SR_Guess5) if ((CO2_land$SumSqResidRegression[1]&lt;CO2_land$SSR_Guess1[1]) &amp; (CO2_land$SumSqResidRegression[1]&lt;CO2_land$SSR_Guess2[1]) &amp; (CO2_land$SumSqResidRegression[1]&lt;CO2_land$SSR_Guess3[1]) &amp; (CO2_land$SumSqResidRegression[1]&lt;CO2_land$SSR_Guess4[1]) &amp; (CO2_land$SumSqResidRegression[1]&lt;CO2_land$SSR_Guess5[1])){ print(&quot;Yes, as it is mathematically necessary, the SSR for the mathematically estimated model is the lowest across all estimated scenarios! :)&quot;) } ## [1] &quot;Yes, as it is mathematically necessary, the SSR for the mathematically estimated model is the lowest across all estimated scenarios! :)&quot; Notice that some few municipalities have numbers of voters that are very high compared to the typical values. As a consequence, the regression line is very influenced by those observations (i.e., small displacements in the area where there are many observations have a lower influence on the line than displacements of the same magnitude and direction in the area with few observations). As a consequence, you want to analyze how do the previous estimates change when you restrict your analysis to municipalities with less than 50K eligible voters. Estimate the regression and compare the respective parameters of the models. What slope is higher? What intercept is higher? How does obviating those observations influence our parameters? CO2_land_filtered &lt;- CO2_land %&gt;% filter(anzahlStimmberechtigte &lt; 50000) result2 &lt;- lm(stimmbeteiligungInProzent ~ anzahlStimmberechtigte , data = CO2_land_filtered) summary(result2) ## ## Call: ## lm(formula = stimmbeteiligungInProzent ~ anzahlStimmberechtigte, ## data = CO2_land_filtered) ## ## Residuals: ## Min 1Q Median 3Q Max ## -27.3197 -4.6709 0.2374 4.9221 29.8830 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.698e+01 1.997e-01 335.3 &lt;2e-16 *** ## anzahlStimmberechtigte -8.762e-04 5.093e-05 -17.2 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.62 on 2123 degrees of freedom ## Multiple R-squared: 0.1224, Adjusted R-squared: 0.122 ## F-statistic: 296 on 1 and 2123 DF, p-value: &lt; 2.2e-16 coeffs2&lt;- coef(result2) coeffs2 ## (Intercept) anzahlStimmberechtigte ## 66.9791169423 -0.0008762278 if ((coeffs1[1]&gt;coeffs2[1]) &amp; (coeffs1[1]!=coeffs2[1])){ print(&quot;The original model has a higher intercept than the one only on the restricted observations.&quot;) } else{ print(&quot;The model only on the restricted observations has a higher intercept than the original one.&quot;) } ## [1] &quot;The model only on the restricted observations has a higher intercept than the original one.&quot; if ((coeffs1[2]&gt;coeffs2[2]) &amp; (coeffs1[2]!=coeffs2[2])){ print(&quot;The original model has a higher slope than the one only on the restricted observations.&quot;) } else{ print(&quot;The model only on the restricted observations has a higher slope than the original one.&quot;) } ## [1] &quot;The original model has a higher slope than the one only on the restricted observations.&quot; The reason the parameters changed is because the new model needs to fit data that is concentrated in a more clearly negative relation between our variables of interest. [Optional] Modify slightly the parameters estimated using the linear regression. For example, keeping the intercept unchanged, increase or decrease the slope in 1 percent every time. Now, compare the sum of squared residuals. Do your experiments show smaller SSR? Is it possible? Finally, you want to analyze a counterfactual scenario: What would have been the outcome of the referendum should the turn-out in each of the municipalities would have been as high as in Zurich? OriginalOutcome = sum(CO2_land$jaStimmenAbsolut)/(sum(CO2_land$jaStimmenAbsolut)+sum(CO2_land$neinStimmenAbsolut)) CO2_land$Valid_TurnOut =CO2_land$gueltigeStimmen/CO2_land$anzahlStimmberechtigte CO2_land$OriginalOutcome_test = CO2_land$jaStimmenInProzent * CO2_land$anzahlStimmberechtigte * CO2_land$Valid_TurnOut /100 # Min TurnOut_min= min(CO2_land$Valid_TurnOut) CO2_land$Yes_Conterfactual_min= CO2_land$jaStimmenInProzent * CO2_land$anzahlStimmberechtigte * TurnOut_min /100 CO2_land$No_Conterfactual_min= (100-CO2_land$jaStimmenInProzent) * CO2_land$anzahlStimmberechtigte * TurnOut_min /100 CounterfactualOutcome_min= sum(CO2_land$Yes_Conterfactual_min)/(sum(CO2_land$Yes_Conterfactual_min)+sum(CO2_land$No_Conterfactual_min)) # Max TurnOut_max= max(CO2_land$Valid_TurnOut) CO2_land$Yes_Conterfactual_max= CO2_land$jaStimmenInProzent * CO2_land$anzahlStimmberechtigte * TurnOut_max CO2_land$No_Conterfactual_max= (100-CO2_land$jaStimmenInProzent) * CO2_land$anzahlStimmberechtigte * TurnOut_max CounterfactualOutcome_max= sum(CO2_land$Yes_Conterfactual_max)/(sum(CO2_land$Yes_Conterfactual_max)+sum(CO2_land$No_Conterfactual_max)) # Zurich TurnOut_Zurich= CO2_land$Valid_TurnOut[CO2_land$mun_id==261] CO2_land$Yes_Conterfactual_Zurich= CO2_land$jaStimmenInProzent * CO2_land$anzahlStimmberechtigte * TurnOut_Zurich CO2_land$No_Conterfactual_Zurich= (100-CO2_land$jaStimmenInProzent) * CO2_land$anzahlStimmberechtigte * TurnOut_Zurich CounterfactualOutcome_Zurich= sum(CO2_land$Yes_Conterfactual_Zurich)/(sum(CO2_land$Yes_Conterfactual_Zurich)+sum(CO2_land$No_Conterfactual_Zurich)) # Luzern TurnOut_Luzern= CO2_land$Valid_TurnOut[CO2_land$mun_id==1061] CO2_land$Yes_Conterfactual_Luzern= CO2_land$jaStimmenInProzent * CO2_land$anzahlStimmberechtigte * TurnOut_Luzern CO2_land$No_Conterfactual_Luzern= (100-CO2_land$jaStimmenInProzent) * CO2_land$anzahlStimmberechtigte * TurnOut_Luzern CounterfactualOutcome_Luzern= sum(CO2_land$Yes_Conterfactual_Luzern)/(sum(CO2_land$Yes_Conterfactual_Luzern)+sum(CO2_land$No_Conterfactual_Luzern)) cat(&#39;The original percentage for Yes was:&#39;, OriginalOutcome, &quot;\\nThe counterfactual using the minimum turnout would have been: &quot;, CounterfactualOutcome_min, &quot;\\nThe counterfactual using the maximum turnout would have been: &quot;, CounterfactualOutcome_max, &quot;\\nThe counterfactual using Zurich\\&#39;s turnout would have been: &quot;, CounterfactualOutcome_Zurich, &quot;\\nThe counterfactual using Luzern\\&#39;s turnout would have been: &quot;, CounterfactualOutcome_Luzern) ## The original percentage for Yes was: 0.4812683 ## The counterfactual using the minimum turnout would have been: 0.4854131 ## The counterfactual using the maximum turnout would have been: 0.4854131 ## The counterfactual using Zurich&#39;s turnout would have been: 0.4854131 ## The counterfactual using Luzern&#39;s turnout would have been: 0.4854131 if ((CounterfactualOutcome_min &gt;.5) | (CounterfactualOutcome_max &gt;.5) | (CounterfactualOutcome_Zurich &gt;.5) | (CounterfactualOutcome_Luzern &gt;.5)){ print(&#39;There is at least one counterfactual where the outcome changes&#39;) }else{ print(&#39;There is no counterfactual where the outcome changes&#39;) } ## [1] &quot;There is no counterfactual where the outcome changes&quot; Given your preliminary analysis, you find that, on average, municipalities with bigger populations had a lower turn-out than those with smaller ones. To bring some more nuance to your analysis and capture how rural a municipality is, you use the agricultural area of the municipality (AgriculturalAreaHa) as a regressor (aka. independent variable) for the same regressand (aka. dependent variable) of the previous point (i.e., Turn-out in percent). Report and interpret the parameter. Also present the corresponding graph. Write the regression here. result &lt;- lm(stimmbeteiligungInProzent ~ AgriculturalAreaHa , data = CO2_land) summary(result) ## ## Call: ## lm(formula = stimmbeteiligungInProzent ~ AgriculturalAreaHa, ## data = CO2_land) ## ## Residuals: ## Min 1Q Median 3Q Max ## -25.189 -5.328 0.202 5.545 31.679 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.445e+01 2.128e-01 302.902 &lt; 2e-16 *** ## AgriculturalAreaHa 8.477e-04 1.772e-04 4.784 1.84e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.092 on 2130 degrees of freedom ## Multiple R-squared: 0.01063, Adjusted R-squared: 0.01016 ## F-statistic: 22.88 on 1 and 2130 DF, p-value: 1.84e-06 Report and interpret the parameter here. coeffs&lt;- coef(result) coeffs ## (Intercept) AgriculturalAreaHa ## 6.444848e+01 8.476927e-04 The estimated (Intercept) is 64.45, while the estimated slope for AgriculturalAreaHa is 0. It means that, across Swiss municipalities, an increase of a thousand voters was associated with an average decrease in the turn-out of 0.85 percentage points. Present the graph here. # Create a scatter plot ggplot(CO2_land, aes(x = AgriculturalAreaHa, y = stimmbeteiligungInProzent)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x , se = FALSE, color = &quot;purple&quot;) + # Adding the regression line theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Agricultural Area (Ha)&quot;, y = &quot;Turn-out (%)&quot;, title = &quot;Scatter plot with Linear Regression Line&quot; ) The result from the previous point is consistent with the result of the vote given the urban-rural divide in preferences (i.e., municipalities with more agricultural area had higher participation rates). However, you notice that your comparisons across municipalities are not very rigorous because you are ignoring how important agricultural land is with respect to the whole municipality area. An implication of that is that two municipalities with the same agricultural area could be taken as equally rural while one could have a very big urban area (e.g., Zurich) and the other have a very small one. Thus to make your analysis across more comparable units, you transform the absolute value of the agricultural area to a relative one: the percentage of the municipal area that is agricultural. Create the variable AgricAreaPercent to represent those transformed values. Regress the turn-out on the newly created variable. Again, do the regression without intercept. Report and interpret the parameter. Also present the corresponding graph. Write the regression here. #Transform the units to make them comparable CO2_land &lt;- CO2_land %&gt;% mutate(AgricAreaPercent= 100*AgriculturalAreaHa/TotalAreaHa) #regression result0 &lt;- lm(stimmbeteiligungInProzent ~ AgricAreaPercent , data = CO2_land) summary(result0) ## ## Call: ## lm(formula = stimmbeteiligungInProzent ~ AgricAreaPercent, data = CO2_land) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.971 -4.760 -0.270 4.314 39.145 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 55.189867 0.384603 143.5 &lt;2e-16 *** ## AgricAreaPercent 0.219491 0.007894 27.8 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.969 on 2130 degrees of freedom ## Multiple R-squared: 0.2663, Adjusted R-squared: 0.2659 ## F-statistic: 773.1 on 1 and 2130 DF, p-value: &lt; 2.2e-16 Report and interpret the parameter here. coeffs&lt;- coef(result0) coeffs ## (Intercept) AgricAreaPercent ## 55.189867 0.219491 The estimated (Intercept) is 55.19, while the estimated slope for AgricAreaPercent is 0.22. It means that, across Swiss municipalities an increase of one percentage point in the share of agricultural land was associated with an average increase in the turn-out of 0.22 percentage points. Present the graph here. # Create a scatter plot ggplot(CO2_land, aes(x = AgricAreaPercent, y = stimmbeteiligungInProzent)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;purple&quot;) + # Adding the regression line theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Agricultural Area (%)&quot;, y = &quot;Turn-out (%)&quot;, title = &quot;Scatter plot with Linear Regression Line&quot; ) [Optional] While analyzing your results, you asked yourself: what would have happened with my regression if it is estimated without an intercept? In particular, you asked yourself: does the the distance from each data point to the regression line decrease when an intercept is used? See this entry, particularly the graph with the blue and red squares to understand the logic of the R^2 as a measure for goodness of fit. You want to compare both the regression lines with and without the intercept in one same graph. Do it. What lesson can you get from this comparison? ggplot(CO2_land, aes(x = AgricAreaPercent, y = stimmbeteiligungInProzent)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x-1, se = FALSE, color = &quot;red&quot;) + # Adding the regression line geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;purple&quot;) + # Adding the regression line scale_y_continuous(limits = c(25, max(CO2_land$stimmbeteiligungInProzent, na.rm = TRUE))) + # theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Agricultural Area (%)&quot;, y = &quot;Turn-out (%)&quot;, title = &quot;Scatter plot with Linear Regression Line&quot; ) The lesson is that using the intercept is always desirable as it can potentially improve the goodness of fit, while doesn’t worsen it (i.e., the optimal intercept could be estimated to be zero). As it is clear from the graph, not using the intercept provides less reliable predictions and general descriptions for the relation between our variables of interest. [Optional] Now add a vertical line with the average for the agricultural area, and a horizontal one with the average turn-out. What does this new graph tell you about the descriptive performance of a regression with and without intercepts? #Means for Y and X mean_AgricAreaPercent &lt;- mean(CO2_land$AgricAreaPercent, na.rm = TRUE) mean_stimmbeteiligungInProzent &lt;- mean(CO2_land$stimmbeteiligungInProzent, na.rm = TRUE) # Create a scatter plot ggplot(CO2_land, aes(x = AgricAreaPercent, y = stimmbeteiligungInProzent)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x - 1, se = FALSE, color = &quot;red&quot;) + # Adding the regression line geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;purple&quot;) + # Adding the regression line geom_hline(yintercept = mean_stimmbeteiligungInProzent, linetype = &quot;dashed&quot;, color = &quot;green&quot;) + # Horizontal line at the mean of y geom_vline(xintercept = mean_AgricAreaPercent, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + # Vertical line at the mean of x scale_y_continuous(limits = c(25, max(CO2_land$stimmbeteiligungInProzent, na.rm = TRUE))) + # theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Agricultural Area (%)&quot;, y = &quot;Turn-out (%)&quot;, title = &quot;Scatter plot with Linear Regression Line&quot; ) The descriptive performance of a regression with an intercept is better than without it. As the graph shows, the vertical and horizontal lines for the averages of the dependent and independent variables overlap with each other in exactly the same place where the regression line with intercept goes through. This shows the capability of single linear regression to capture central tendency of the fitted data. On the contrary, the regression without intercept does not do that. Once again, this shows how the descriptive performance of a regression with intercept captures more intuitively the general relation between one variable and another. Store the residuals as residuals. Plot the share of agricultural area against the residuals. What does it tell you regarding the ability of the model to describe the central tendency of the relationship across all values of the agricultural land share? residuals0 &lt;- residuals(result0) ggplot(CO2_land, aes(x = AgricAreaPercent, y = residuals0)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x , se = TRUE, color = &quot;green&quot;)+ # Adding the theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Agricultural Area (%)&quot;, y = &quot;Residuals (turn-out p.p.)&quot;, title = &quot;Scatter plot with Linear Regression Line&quot; ) The existence of a horizontal slope between the residuals (i.e., the distance between the prediction and the real value) and the agricultural share suggests that our model has systematically captured the relation between our variables: the average error does not not change systematically across the values of the agricultural share. [Optional] Repeat the previous point without the intercept. Does the linear regression remains a good tool for describing the central tendency of the relationship across all values of the agricultural land share? #regression result1 &lt;- lm(stimmbeteiligungInProzent ~ AgricAreaPercent -1 , data = CO2_land) summary(result1) ## ## Call: ## lm(formula = stimmbeteiligungInProzent ~ AgricAreaPercent - 1, ## data = CO2_land) ## ## Residuals: ## Min 1Q Median 3Q Max ## -42.102 -7.509 6.449 23.631 83.331 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## AgricAreaPercent 1.26144 0.01012 124.7 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 22.75 on 2131 degrees of freedom ## Multiple R-squared: 0.8795, Adjusted R-squared: 0.8794 ## F-statistic: 1.555e+04 on 1 and 2131 DF, p-value: &lt; 2.2e-16 residuals1 &lt;- residuals(result1) ggplot(CO2_land, aes(x = AgricAreaPercent, y = residuals1)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x , se = TRUE, color = &quot;red&quot;)+ # Adding the theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Agricultural Area (%)&quot;, y = &quot;Residuals (turn-out p.p.)&quot;, title = &quot;Scatter plot with Linear Regression Line&quot; ) Unlike the graph of the previous point showed that no systematic relation between the residual and our regressor was not captured by our model, the graph of this point shows a systematic behavior of the residuals. In this case, it is the fact that the model is too inflexible (i.e., the line must always go through the origin) which leads the model to predict the values of our regressand in a systematically mistaken way: the average error does change systematically across the values of the agricultural share, being systematically positive for lower age values, while systematically negative for high age values. 3.3.2 Simulated data You will now simulate the relationship between turn-out and share of agricultural land. Assume the parameters estimated in the last regression (i.e., the \\(\\beta_0\\) and \\(\\beta_1\\)). Retrieve both parameters. coeffs&lt;- coef(result0) print(coeffs) ## (Intercept) AgricAreaPercent ## 55.189867 0.219491 print(unname(coeffs[1]), digits = 2) ## [1] 55 print(unname(coeffs[2]), digits = 2) ## [1] 0.22 Use the parameters to define the assumed data generating process. Assume that the process is deterministic (i.e., it has no stochastic component). Express the equation using mathematical notation. \\[TurnOut_i = \\beta_0 + \\beta_1 * ShareAgriculturalLand\\] Generate a normal distribution that preserves the mean and standard deviation of the original data on share of agricultural land. set.seed(123) # to ensure reproducibility: should different people use different random draws, the results won&#39;t be consistent. Thus, should all use the same random draw, all should have the same results. CO2_land$Sim_ShareAgric_norm &lt;- rnorm(nrow(CO2_land), mean = mean_AgricAreaPercent, sd = sd(CO2_land$AgricAreaPercent)) hist(CO2_land$Sim_ShareAgric_norm, main=&quot;Histogram of Share of Simulated Agricultural Land&quot;, xlab=&quot;Data Values&quot;, ylab=&quot;Frequency&quot;, col=&quot;blue&quot;, border=&quot;black&quot;) abline(v=mean_AgricAreaPercent, col=&quot;red&quot;, lwd=2, lty=2) Predict the turn-out values given the simulated inputs on the share of agricultural land using the data generating process defined in point i). Graph relation between both variables using a liner regression line. CO2_land$Pred_TurnOut_Determ &lt;- unname(coeffs[1]) + unname(coeffs[2])*CO2_land$Sim_ShareAgric_norm ggplot(CO2_land, aes(x = Sim_ShareAgric_norm, y = Pred_TurnOut_Determ)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;purple&quot;) + # Adding the regression line geom_hline(yintercept = mean_stimmbeteiligungInProzent, linetype = &quot;dashed&quot;, color = &quot;green&quot;) + # Horizontal line at the mean of y geom_vline(xintercept = mean_AgricAreaPercent, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + # Vertical line at the mean of x scale_y_continuous(limits = c(25, max(CO2_land$stimmbeteiligungInProzent, na.rm = TRUE))) + # theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Agricultural Area (%)&quot;, y = &quot;Turn-out (%)&quot;, title = &quot;Simulation: deterministic model&quot; ) As expected, this deterministic model generates points on top of the regression line defined by the intercept and slope of the data generating process. Unlike in reality, we have no random errors generating random deviations from the regression line (i.e., we have no cloud of points, but they are all on top of the line). Generate a normal distribution with error distribution with mean zero and standard deviation equal to the standard deviation of the residual of the model with intercept. set.seed(123) CO2_land$Sim_resid_norm &lt;- rnorm(nrow(CO2_land), mean = 0, sd = sd(residuals0)) hist(CO2_land$Sim_resid_norm, main=&quot;Histogram of Simulated Normal Residuals&quot;, xlab=&quot;Data Values&quot;, ylab=&quot;Frequency&quot;, col=&quot;blue&quot;, border=&quot;black&quot;) abline(v=0, col=&quot;red&quot;, lwd=2, lty=2) Now add the stochastic element to the fully deterministic model of point iv). Graph the relation between both variables using a liner regression line. CO2_land$Pred_TurnOut_Stoch &lt;- CO2_land$Pred_TurnOut_Determ + CO2_land$Sim_resid_norm ggplot(CO2_land, aes(x = Sim_ShareAgric_norm, y = Pred_TurnOut_Stoch)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;purple&quot;) + # Adding the regression line geom_hline(yintercept = mean_stimmbeteiligungInProzent, linetype = &quot;dashed&quot;, color = &quot;green&quot;) + # Horizontal line at the mean of y geom_vline(xintercept = mean_AgricAreaPercent, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + # Vertical line at the mean of x scale_y_continuous(limits = c(25, max(CO2_land$Sim_ShareAgric_norm, na.rm = TRUE))) + # theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Agricultural Area (%)&quot;, y = &quot;Turn-out (%)&quot;, title = &quot;Simulation: deterministic and stochastic model&quot; ) The previous graph presents the relation between our simulated independent variable and residual, both of which fully generate our dependent variable (i.e., only using the regression parameters to predict the observations over the line and then adding the residuals we can generate all the values for our dependent variable). [Optional] Now repeat the same simulation but generating errors that distribute uniformly. Use the maximum, and minimum values of the residuals for the model with intercept to define the distribution of residuals. Uniform simulated residuals: set.seed(123) CO2_land$Sim_resid_unif &lt;- runif(nrow(CO2_land), min = min(residuals0), max = max(residuals0)) hist(CO2_land$Sim_resid_unif, main=&quot;Histogram of Simulated Uniform Residuals&quot;, xlab=&quot;Data Values&quot;, ylab=&quot;Frequency&quot;, col=&quot;blue&quot;, border=&quot;black&quot;) abline(v=0, col=&quot;red&quot;, lwd=2, lty=2) Data generating process with uniform residuals: CO2_land$Pred_TurnOut_Unif &lt;- CO2_land$Pred_TurnOut_Determ + CO2_land$Sim_resid_unif ggplot(CO2_land, aes(x = Sim_ShareAgric_norm, y = Pred_TurnOut_Unif)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;purple&quot;) + geom_hline(yintercept = mean(CO2_land$Pred_TurnOut_Unif), linetype = &quot;dashed&quot;, color = &quot;green&quot;) + geom_vline(xintercept = mean_AgricAreaPercent, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + scale_y_continuous(limits = c(25, max(CO2_land$Pred_TurnOut_Unif, na.rm = TRUE))) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Agricultural Area (%)&quot;, y = &quot;Turn-out (%)&quot;, title = &quot;Simulation: deterministic and stochastic model (uniform error)&quot; ) 3.4 Appendix: Getting the data first To get the data, you will use an API (application programming interface). Put plainly, for the sake of this exercise you can think of an API like a waiter to whom you make a request and who gets what ever you requested from the kitchen (a remote data source). In this case, you will use two APIs: to get election results in Switzerland, swissdd, and BFS to get many other kinds of statistical information. Both APIs get information from the Federal Statistical Office (Bundesamt für Statistik). Skimm the functions of the Swissdd package. Then, inspect the codebook to understand what for the columns and rows stand for. Do not use more than 5 minutes doing it. To get the data, simply execute the following code. We present you with the code so, if you want, you can know how it works and you can play around with it on your own following the code’s logic. Install the Swissdd package #First option - from CRAN. If you install it from the github repository, make sure you have the `devtools` installed in advance. #install.packages(&quot;swissdd&quot;) #Second option # install.packages(&quot;devtools&quot;) # it is necessary to comment the install.packages so the book can be rendered devtools::install_github(&quot;politanch/swissdd&quot;, force = TRUE) ## ## ── R CMD build ───────────────────────────────────────────────────────────────── ## * checking for file ‘/private/var/folders/fg/p53z0kxs5g35bp_2wq9x5vfdsyrmgw/T/Rtmp8kAvOf/remotesc83113cf314/politanch-swissdd-04bb42c/DESCRIPTION’ ... OK ## * preparing ‘swissdd’: ## * checking DESCRIPTION meta-information ... OK ## * checking for LF line-endings in source and make files and shell scripts ## * checking for empty or unneeded directories ## * building ‘swissdd_1.1.5.tar.gz’ ## Warning: invalid uid value replaced by that for user &#39;nobody&#39; ## Warning: invalid gid value replaced by that for user &#39;nobody&#39; # library(swissdd) Retrieve the CO2 Act information using get_nationalvotes. The vote took place in 13/06/2021. CO2&lt;-get_nationalvotes(votedates = &quot;2021-06-13&quot;) As the same day of the CO2 Act vote more initiatives were voted, keep only the one we care about for this exercise. Notice that each initiative has a numerical identifier called id. Here you can find the numerical identifier under Vote Nº. Keep in mind that, the variable id has the same number as in Vote Nº with an additional zero to the right. CO2&lt;- CO2 %&gt;% filter(id==6440) The resulting data frame contains rows representing the results for a particular vote in a particular municipality. The columns qualify that vote. Use the colnames() function to see the names of the columns. Keep only those that might be relevant for your analysis. CO2&lt;- CO2 %&gt;% select(canton_id, canton_name, mun_id, mun_name, jaStimmenInProzent, jaStimmenAbsolut, neinStimmenAbsolut, stimmbeteiligungInProzent, eingelegteStimmzettel, anzahlStimmberechtigte, gueltigeStimmen) Finally, as you want to see the turn-out differences between municipalities with different degrees of agriculture intensity you need to get some additional data. Run the following code. # install.packages(&quot;BFS&quot;) # library(BFS) #To see the information available in German # catalog_data_de &lt;- bfs_get_catalog_data(language = &quot;de&quot;) #To see the information available in English #catalog_data_en &lt;- bfs_get_catalog_data(language = &quot;en&quot;) #To see the information available in German that contains a particular word in the title #catalog_data_de &lt;- bfs_get_catalog_data(language = &quot;de&quot;, title=&quot;Gemeinde&quot;) #To get the asset number (i.e., numerical id for a data set) # asset_number &lt;- catalog_data_de %&gt;% # filter(title == &quot;Arealstatistik: Standardnomenklatur (NOAS04) nach Bezirk und Gemeinde, in Hektaren&quot;) %&gt;% # pull(number_asset) #Using that asset number, the metadata (i.e., data that describes the data set) can be accessed. #asset_meta &lt;- bfs_get_asset_metadata(number_asset = asset_number) #From the metadata, the bfs number (i.e., a alphanumeric id for the data set) can be accessed. #bfs_number &lt;- asset_meta$shop$orderNr #Finally, using the bfs number the data set can be accessed. #LandUse &lt;- bfs_get_data(number_bfs = bfs_number) #If there is a “Too Many Requests” error message, you can follow this (https://github.com/lgnbhl/BFS#too-many-requests-error-message) #or download the PX file from here https://www.bfs.admin.ch/bfs/de/home/statistiken/kataloge-datenbanken/daten.assetdetail.24865343.html #install.packages(&quot;pxR&quot;) # library(pxR) #Open data LandUse &lt;- read.px(&#39;Data/px-x-0202020000_102.px&#39;, encoding = &quot;UTF-8&quot;) LandUseData&lt;- LandUse$DATA$value #Rename variable LandUseData &lt;- LandUseData %&gt;% rename_with(~ &#39;mun_name&#39;, 3) #Keep the data relevant for my analysis LandUseData&lt;- LandUseData %&gt;% filter(Periode == &quot;2013/18&quot;) %&gt;% filter(Standardnomenklatur..NOAS04.==&quot;-b Landwirtschaftsflächen&quot; | Standardnomenklatur..NOAS04.==&quot;Fläche Total&quot;) %&gt;% filter(str_starts(str_trim(mun_name), fixed(&quot;.&quot;))) %&gt;% mutate(mun_name = str_replace_all(mun_name, fixed(&quot;......&quot;), &quot;&quot;)) # install.packages(&quot;tidyverse&quot;) #Adjust data from long to wide LandUseData &lt;- LandUseData %&gt;% pivot_wider(names_from = Standardnomenklatur..NOAS04., values_from = value) #Rename variables LandUseData &lt;- LandUseData %&gt;% rename_with(~ &#39;TotalAreaHa&#39;, 3) %&gt;% rename_with(~ &#39;AgriculturalAreaHa&#39;, 4) %&gt;% select(-Periode) Using the additional data, you can now merge both data sets so you can know how did voters behave in more agricultural areas. Use the inner_join function. The numerical IDs are available in LandUseData but not in CO2. However, as both sources share the mun_name, you use that variable to match them. CO2 &lt;- inner_join(CO2, LandUseData, by=&quot;mun_name&quot;) write.csv(CO2, &quot;Data/CO2_land.csv&quot;) "],["week-4-regression-ii-model-specification.html", "Chapter 4 Week 4: Regression II (Model Specification) 4.1 Aims 4.2 Exercise: Context and questions 4.3 Adjusting the data first 4.4 Exercise: solution", " Chapter 4 Week 4: Regression II (Model Specification) 4.1 Aims Using multiple regressors and interaction terms in regression analysis for quantitative descriptive purposes with real data Using simulation to instantiate how different properties of the data generating process alter or not metrics of goodness of fit 4.2 Exercise: Context and questions Currently led by Prof. Anke Tresch, “[t]he Swiss Election Study (Selects) has been investigating the electoral behaviour of Swiss citizens in national elections since 1995. The project sheds light on the dynamics of the citizens’ opinion formation as well as on the determinants of their political participation and voting choice for a specific candidate or party.” See more here. In this exercise, you will use data for the year 2019. You will use it to answer the following questions: Do older voters have stronger right leaning preferences than younger ones? Do political preferences vary across sexes? Do political preferences vary across the main language spoken at the respondent’s home? Do differences in political preferences across sexes vary depending on the respondents’ age? Do differences in political preferences across sexes vary depending on the respondents’ main language? Do differences in political preferences across main languages vary depending on the respondents’ age? Do differences in political preferences across sexes vary depending on the respondents’ age and main language? 4.3 Adjusting the data first In the folder SELECTS 2019, you can find the file data.csv. It is a version of the SELECTS data for 2019 only with the variables relevant for our exercise. Open the file: selects19 &lt;- read.csv(&quot;Data/SELECTS 2019/data.csv&quot;, header = TRUE) In order to store information in smaller files, researchers use codebooks to numerically represent values that, otherwise, would be much more space consuming in their respective representation in text form. Use the file Selects2019_Codebook_EN.pdf to create the variable LanguageHome where the numerical values for the variable f20221 (i.e., “Main language spoken in family or at home”) are preplaced with their respective text values. selects19 &lt;- selects19 %&gt;% mutate(LanguageHome = case_when( #a new variable is created #each numerical value in the codebook is replace with it&#39;s corresponding text value f20221 == 1 ~ &quot;German/Swiss German&quot;, f20221 == 2 ~ &quot;French&quot;, f20221 == 3 ~ &quot;Italian&quot;, f20221 == 4 ~ &quot;Romansh&quot;, f20221 == 5 ~ &quot;Other&quot;)) As we have seen in week 1, it is a good practice to name your variables so their interpretation is easy. For example, instead of using sex as a variable name, using female facilitates understanding that a zero represents male while a one represents female. Again, use the codebook to transform the variable sex into female. Relabel the sex variable: selects19 &lt;- selects19 %&gt;% mutate(sex = case_when( sex == 1 ~ 0, sex == 2 ~ 1)) Rename the variable sex as female. Take the chance and rename f15200 (i.e., “Left-right placement - Self”) as LeftToRight. selects19 &lt;- dplyr::rename(selects19, LeftToRight = f15200, female = sex) Print the name of all the variables available so you can use them later on. names(selects19) ## [1] &quot;female&quot; &quot;age&quot; &quot;LeftToRight&quot; &quot;f20221&quot; &quot;LanguageHome&quot; 4.4 Exercise: solution Note: Remember that you can copy the code from one point to answer another point. Simply make the necessary adjustments. 4.4.1 Real data Graph age vs. political preferences. Use a linear regression line to describe how the variables relate to each other. #Means for Y and X: to have a clearer notion on the center of mass of the data. mean_LeftRight &lt;- mean(selects19$LeftToRight, na.rm = TRUE) mean_age &lt;- mean(selects19$age, na.rm = TRUE) # Create a scatter plot ggplot(selects19, aes(x = age, y = LeftToRight)) + geom_point() + # Adding the regression line geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;red&quot;) + # Horizontal line at the mean of y geom_hline(yintercept = mean_LeftRight, linetype = &quot;dashed&quot;, color = &quot;green&quot;) + # Vertical line at the mean of x geom_vline(xintercept = mean_age, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Age (years)&quot;, y = &quot;Political Preferences (left to right)&quot;, title = &quot;Scatter plot Age vs. Political Preferences&quot; ) Therefore, we can say that, indeed, older voters have stronger right leaning preferences than younger ones. Since the political preferences as well as years are defined in discrete values, many values can overlap for the same pair(preference, age). Use the ggridges package to see how do preferences distribute across different age values. ggplot(selects19, aes(x = age, y = as.factor(LeftToRight), fill = as.factor(LeftToRight))) + # to show the age distribution for each level of political preference geom_density_ridges() + geom_smooth(data = selects19, aes(x = age, y = LeftToRight, group = 1), method = &quot;lm&quot;, formula = y ~ x, color = &quot;red&quot;, se = FALSE) + geom_hline(yintercept = mean_LeftRight, linetype = &quot;dashed&quot;, color = &quot;green&quot;) + geom_vline(xintercept = mean_age, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + scale_fill_viridis_d() + theme_minimal() + theme(legend.position = &quot;none&quot;) + theme(plot.title = element_text(hjust = 0.5)) + labs(x = &quot;Age (years)&quot;, y = &quot;Political Preferences (left to right)&quot;, title = &quot;Political preferences by age level&quot;, fill = &quot;factor(LeftToRight)&quot;) Regress the political preferences on age. As we learnt in the exercises from last week, the regression with intercept fits data better in most cases: act accordingly. Report and interpret the parameters. Graph some of the main components of the regression output. Write the regression here. #regression result &lt;- lm(LeftToRight ~ age, data = selects19) summary(result) ## ## Call: ## lm(formula = LeftToRight ~ age, data = selects19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.9150 -2.0380 -0.0186 2.1958 5.5271 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.122055 0.106375 38.750 &lt;2e-16 *** ## age 0.019489 0.001979 9.846 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.649 on 5905 degrees of freedom ## Multiple R-squared: 0.01615, Adjusted R-squared: 0.01598 ## F-statistic: 96.94 on 1 and 5905 DF, p-value: &lt; 2.2e-16 Report and interpret the parameters here. coeffs&lt;- coef(result) coeffs ## (Intercept) age ## 4.12205528 0.01948902 The beta value for (Intercept) is 4.12, and for rnames(coeffs[2])` is 0.02. It means that, given our model, an increase of one year in age across the Swiss citizens is associated with an average increase in the LeftToRight self-declared scale of 0.02 ideological units. Likewise, , given our model, newborns can be expected to have an average ideological value of 4.12, while 100 year old citizens can be expected to have an average ideological value of 6.12. We must, nonetheless, remain cautious and keep our predictions as much as possible within the range of regressors our sample provides us with. Graph some of the main components of the regression output here. ggplot(selects19, aes(x = age, y = LeftToRight)) + # Removed selects19$ from aes(), not necessary geom_point(size = 0.001) + geom_abline(aes(intercept = unname(coeffs[1]), slope = 0, color = &quot;Newborn&quot;)) + geom_abline(aes(intercept = unname(coeffs[1]), slope = unname(coeffs[2]), color = &quot;Average individual over time&quot;)) + # Define colors for the legend scale_color_manual(name = &quot;Lines&quot;, values = c(&quot;Newborn&quot; = &quot;gray&quot;, &quot;Average individual over time&quot; = &quot;red&quot;)) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Age (years)&quot;, y = &quot;Political Preferences (left to right)&quot;, title = &quot;Relationship political preferences across ages&quot; ) Now regress LeftToRight on age and female. do the same as in the previous point. Report and interpret the parameters. Graph some of the main components of the regression output. Write the regression here. #regression result &lt;- lm(LeftToRight ~ age + female, data = selects19) summary(result) ## ## Call: ## lm(formula = LeftToRight ~ age + female, data = selects19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.2108 -1.9784 0.0954 2.1145 5.8236 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.492964 0.112519 39.931 &lt;2e-16 *** ## age 0.018672 0.001966 9.496 &lt;2e-16 *** ## female -0.652678 0.068477 -9.531 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.629 on 5904 degrees of freedom ## Multiple R-squared: 0.03106, Adjusted R-squared: 0.03073 ## F-statistic: 94.63 on 2 and 5904 DF, p-value: &lt; 2.2e-16 Report and interpret the parameters here. coeffs&lt;- coef(result) coeffs ## (Intercept) age female ## 4.49296422 0.01867214 -0.65267756 The beta value for (Intercept), ’ is 4.49, for age, is 0.02, and for female is -0.65. It means that, given our model, an increase of one year in age across the Swiss citizens is associated with an average increase in the LeftToRight self-declared scale of 0.02, ideological units. Likewise, male newborns can be expected to have an average ideological value of 4.49, while 100 year old male citizens can be expected to have an average ideological value of 6.49. Similarly, for each of those scenarios, should the individual be a female, it can be expected to have an average of 0.65 ideological units lower than a male individual under the same statistical circumstances. In other words, by design, our particular model only captures the average evolution of ideology over time but does not distinguish whether nor how it varies across sexes. Finally, just as in the previous point, we must remain cautious and keep our predictions as much as possible within the range of regressors our sample provides us with. Graph some of the main components of the regression output here. ggplot(selects19, aes(x = age, y = LeftToRight)) + geom_point(size = 0.001) + geom_abline(aes(intercept = unname(coeffs[1]), slope = 0, color = &quot;Male newborn&quot;)) + geom_abline(aes(intercept = unname(coeffs[1]) + unname(coeffs[3]), slope = 0, color = &quot;Female newborn&quot;)) + geom_abline(aes(intercept = unname(coeffs[1]), slope = unname(coeffs[2]), color = &quot;Male over time&quot;)) + geom_abline(aes(intercept = unname(coeffs[1]) + unname(coeffs[3]), slope = unname(coeffs[2]), color = &quot;Female over time&quot;)) + scale_color_manual(name = &quot;Lines&quot;, values = c(&quot;Male newborn&quot; = &quot;gray&quot;, &quot;Female newborn&quot; = &quot;red&quot;, &quot;Male over time&quot; = &quot;orange&quot;, &quot;Female over time&quot; = &quot;purple&quot;)) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Age (years)&quot;, y = &quot;Political Preferences (left to right)&quot;, title = &quot;Relationship political preferences across ages by sex \\n(without interaction)&quot;, color = &quot;Line Type&quot; ) You now remember that average political preferences across age can change across sexes: i.e., the average preferences across individuals in a same sex can change at different rates over time as well as begin from different departure points. Add the interaction Age and female and do the same as in the previous point. Write the regression here. #regression result &lt;- lm(LeftToRight ~ age * female, data = selects19) summary(result) ## ## Call: ## lm(formula = LeftToRight ~ age * female, data = selects19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.9332 -2.0641 0.0372 2.1848 6.0372 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.847656 0.152489 31.790 &lt; 2e-16 *** ## age 0.011799 0.002801 4.213 2.56e-05 *** ## female -1.340734 0.211254 -6.347 2.37e-10 *** ## age:female 0.013528 0.003930 3.443 0.00058 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.626 on 5903 degrees of freedom ## Multiple R-squared: 0.033, Adjusted R-squared: 0.03251 ## F-statistic: 67.15 on 3 and 5903 DF, p-value: &lt; 2.2e-16 Report and interpret the parameters here. coeffs&lt;- coef(result) coeffs ## (Intercept) age female age:female ## 4.84765642 0.01179933 -1.34073442 0.01352810 The beta value for (Intercept) is 4.85 for age is 0.01 for female is -1.34 and for age:female is 0.01. It means that, given our model, an increase of one year in age across the male Swiss citizens is associated with an average increase in the LeftToRight self-declared scale of 0.01 ideological units. Likewise, while male newborns can be expected to have an average ideological value of 4.85, female newborns can be expected to have an average ideological value of 3.51. Also, while 100 year old male citizens can be expected to have an average ideological value of 5.85, 100 year old females can be expected to have an average ideological value of 5.51. Finally, while, given our model, females have lower initial ideological initial values, the rate at which right leaning preferences increase among them over time is 0.01 ideological units per year higher than in males. Therefore, we can expect males’ and females’ average ideological values to converge when they are around 100 years old. Finally, just as in the previous point, we must remain cautious and keep our predictions as much as possible within the range of regressors our sample provides us with. Graph some of the main components of the regression output here. ggplot(selects19, aes(x = age, y = LeftToRight)) + geom_point(size = 0.001) + geom_abline(aes(intercept = unname(coeffs[1]), slope = 0, color = &quot;Male newborn&quot;)) + geom_abline(aes(intercept = unname(coeffs[1]) + unname(coeffs[3]), slope = 0, color = &quot;Female newborn&quot;)) + geom_abline(aes(intercept = unname(coeffs[1]), slope = unname(coeffs[2]), color = &quot;Male over time&quot;)) + geom_abline(aes(intercept = unname(coeffs[1]) + unname(coeffs[3]), slope = unname(coeffs[2])+unname(coeffs[4]), color = &quot;Female over time&quot;)) + scale_color_manual(name = &quot;Lines&quot;, values = c(&quot;Male newborn&quot; = &quot;gray&quot;, &quot;Female newborn&quot; = &quot;red&quot;, &quot;Male over time&quot; = &quot;orange&quot;, &quot;Female over time&quot; = &quot;purple&quot;)) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Age (years)&quot;, y = &quot;Political Preferences (left to right)&quot;, title = &quot;Relationship political preferences across ages by sex \\n(with interaction)&quot; ) Unlike the previous point, in this point our model did not presume that the rate at which right leaning preferences increase among them over time is shared between males and females. Indeed, what the model shows is that both the intercepts and slopes are different for both groups. Particularly, give our model, the particular mix of intercepts and slopes allow females and males to converge on their average ideological values over time. Following the same logic as in the previous point, you now add the LanguageHome categorical variable with the corresponding interactions. Do the same as in the previous point. Write the regression here. #regression result &lt;- lm(LeftToRight ~ age * female * LanguageHome, data = selects19) summary(result) ## ## Call: ## lm(formula = LeftToRight ~ age * female * LanguageHome, data = selects19) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.1650 -2.0129 0.0157 2.0714 6.1504 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 4.6522028 0.2598442 17.904 ## age 0.0092259 0.0048428 1.905 ## female -1.2217711 0.3489102 -3.502 ## LanguageHomeGerman/Swiss German 0.1982265 0.3407428 0.582 ## LanguageHomeItalian 0.8741812 0.4954660 1.764 ## LanguageHomeOther 0.5409228 0.7585873 0.713 ## LanguageHomeRomansh -1.2787995 2.0100116 -0.636 ## age:female 0.0140639 0.0065181 2.158 ## age:LanguageHomeGerman/Swiss German 0.0055441 0.0062621 0.885 ## age:LanguageHomeItalian -0.0030304 0.0092982 -0.326 ## age:LanguageHomeOther -0.0095835 0.0145811 -0.657 ## age:LanguageHomeRomansh 0.0466591 0.0397951 1.172 ## female:LanguageHomeGerman/Swiss German -0.1885585 0.4684575 -0.403 ## female:LanguageHomeItalian 0.5629460 0.7043389 0.799 ## female:LanguageHomeOther -1.6468317 0.9783983 -1.683 ## female:LanguageHomeRomansh -1.0259620 4.0396347 -0.254 ## age:female:LanguageHomeGerman/Swiss German -0.0002337 0.0086831 -0.027 ## age:female:LanguageHomeItalian -0.0124492 0.0133246 -0.934 ## age:female:LanguageHomeOther 0.0265266 0.0193016 1.374 ## age:female:LanguageHomeRomansh -0.0041536 0.0833735 -0.050 ## Pr(&gt;|t|) ## (Intercept) &lt; 2e-16 *** ## age 0.056817 . ## female 0.000466 *** ## LanguageHomeGerman/Swiss German 0.560759 ## LanguageHomeItalian 0.077723 . ## LanguageHomeOther 0.475833 ## LanguageHomeRomansh 0.524661 ## age:female 0.030994 * ## age:LanguageHomeGerman/Swiss German 0.376011 ## age:LanguageHomeItalian 0.744501 ## age:LanguageHomeOther 0.511041 ## age:LanguageHomeRomansh 0.241051 ## female:LanguageHomeGerman/Swiss German 0.687324 ## female:LanguageHomeItalian 0.424175 ## female:LanguageHomeOther 0.092391 . ## female:LanguageHomeRomansh 0.799525 ## age:female:LanguageHomeGerman/Swiss German 0.978527 ## age:female:LanguageHomeItalian 0.350189 ## age:female:LanguageHomeOther 0.169395 ## age:female:LanguageHomeRomansh 0.960268 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.615 on 5887 degrees of freedom ## Multiple R-squared: 0.04377, Adjusted R-squared: 0.04068 ## F-statistic: 14.18 on 19 and 5887 DF, p-value: &lt; 2.2e-16 Report and interpret the parameters here. coeffs &lt;- coef(result) coeffs ## (Intercept) ## 4.6522027670 ## age ## 0.0092259369 ## female ## -1.2217710646 ## LanguageHomeGerman/Swiss German ## 0.1982265191 ## LanguageHomeItalian ## 0.8741812107 ## LanguageHomeOther ## 0.5409227964 ## LanguageHomeRomansh ## -1.2787994823 ## age:female ## 0.0140639403 ## age:LanguageHomeGerman/Swiss German ## 0.0055440902 ## age:LanguageHomeItalian ## -0.0030304217 ## age:LanguageHomeOther ## -0.0095835495 ## age:LanguageHomeRomansh ## 0.0466590996 ## female:LanguageHomeGerman/Swiss German ## -0.1885585461 ## female:LanguageHomeItalian ## 0.5629459596 ## female:LanguageHomeOther ## -1.6468317018 ## female:LanguageHomeRomansh ## -1.0259619787 ## age:female:LanguageHomeGerman/Swiss German ## -0.0002337215 ## age:female:LanguageHomeItalian ## -0.0124491713 ## age:female:LanguageHomeOther ## 0.0265265501 ## age:female:LanguageHomeRomansh ## -0.0041536238 Interpreting a regression with ever more interactions is exponentially harder! The reason is that the potential scenarios to evaluate must take into account the interaction between the categorical variables of sex (i.e., two potential values) and language (i.e., five potential values) as well as the continuous variable age. In our simplest model with no interaction in point 3 we had only two parameters, in point 4 we had three, in point 5 we had four parameters with one interaction. However, in point 6 we have 20 parameters. Consequently, below we present only some few cases aimed at signaling the general logic in the interpretation: we compare the male and female cases between French and Swiss German individuals. First of all, the reference group are male newborns in French speaking households. For them, given our model, we can expect the average political preference to be 4.65. In contrast, male newborns in Swiss German speaking households can be expected to have 0.2 ideological units more than the reference group, i.e., 4.85. Female newborns in French speaking households can be expected to have -1.22 ideological units less than the reference group, i.e., 3.43. On the other hand, Female newborns in Swiss German speaking households can be expected to have -0.19 ideological units less than the reference group, i.e., 4.46. Likewise, for 100 year old males from French speaking households, we can expect their average political preference value to be 5.57. On the contrary, 100 year old males from Swiss German speaking households can be expected to have 0.75 ideological units more than the reference group, i.e., 6.33. In the case of 100 year old females from French speaking households, we can expect their average political preference value to be 5.76, while for 100 year old females from Swiss German speaking households we can expect their average political preference value to be 5.75. Graph some of the main components of the regression output here. ggplot(selects19, aes(x = age, y = LeftToRight)) + geom_point(size = 0.001) + geom_abline(aes(intercept = unname(coeffs[1]), slope = 0, color = &quot;Male newborn French&quot;), linetype = &quot;dashed&quot;) + geom_abline(aes(intercept = unname(coeffs[1]), slope = unname(coeffs[2]), color = &quot;Male over time French&quot;)) + geom_abline(aes(intercept = unname(coeffs[1]) + unname(coeffs[4]), slope = 0, color = &quot;Male newborn Swiss German&quot;), linetype = &quot;dashed&quot;) + geom_abline(aes(intercept = unname(coeffs[1]) + unname(coeffs[4]), slope = unname(coeffs[2]) + unname(coeffs[9]), color = &quot;Male over time Swiss German&quot;)) + geom_abline(aes(intercept = unname(coeffs[1]) + unname(coeffs[3]), slope = 0, color = &quot;Female newborn French&quot;), size = 2, linetype = &quot;dashed&quot;) + geom_abline(aes(intercept = unname(coeffs[1]) + unname(coeffs[3]), slope = unname(coeffs[2]) + unname(coeffs[8]), color = &quot;Female over time French&quot;), size = 2) + geom_abline(aes(intercept = unname(coeffs[1]) + unname(coeffs[3]) + unname(coeffs[4]) + unname(coeffs[13]), slope = 0, color = &quot;Female newborn Swiss German&quot;), linetype = &quot;dashed&quot;) + geom_abline(aes(intercept = unname(coeffs[1]) + unname(coeffs[3]) + unname(coeffs[4]) + unname(coeffs[13]), slope = unname(coeffs[2]) + unname(coeffs[8]) + unname(coeffs[17]), color = &quot;Female over time Swiss German&quot;)) + scale_color_manual(name = &quot;Lines&quot;, values = c(&quot;Male newborn French&quot; = &quot;blue&quot;, &quot;Male over time French&quot; = &quot;blue&quot;, &quot;Male newborn Swiss German&quot; = &quot;red&quot;, &quot;Male over time Swiss German&quot; = &quot;red&quot;, &quot;Female newborn French&quot; = &quot;orange&quot;, &quot;Female over time French&quot; = &quot;orange&quot;, &quot;Female newborn Swiss German&quot;= &quot;purple&quot;, &quot;Female over time Swiss German&quot;= &quot;purple&quot;)) + scale_linetype_manual(name = &quot;Lines&quot;, values = c(&quot;Male newborn French&quot; = &quot;dashed&quot;, &quot;Male over time French&quot; = &quot;solid&quot;, &quot;Male newborn Swiss German&quot; = &quot;dashed&quot;, &quot;Male over time Swiss German&quot; = &quot;solid&quot;, &quot;Female newborn French&quot; = &quot;dashed&quot;, &quot;Female over time French&quot; = &quot;solid&quot;, &quot;Female newborn Swiss German&quot; = &quot;dashed&quot;, &quot;Female over time Swiss German&quot; = &quot;solid&quot;)) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Age (years)&quot;, y = &quot;Political Preferences (left to right)&quot;, title = &quot;Relationship political preferences across ages by sex and language \\n(with interaction)&quot; ) 4.4.2 Simulated data In order to see the logic behind the \\(R^2\\), you will simulate what happens to it under different circumstances. To begin with, generate a random normal distribution of \\(X\\) and \\(\\epsilon\\) both with mean 0 and standard deviation of 1. set.seed(123) # For reproducibility df &lt;- data.frame(X = rnorm(1000, mean = 0, sd = 1), epsilon = rnorm(100, mean = 0, sd = 1)) Generate values for \\(Y\\) using the following data generating process. Graph it and regress \\(Y\\) on \\(X\\). \\[Y=1+2X+\\epsilon\\] df$Y &lt;- 1+2*df$X+df$epsilon ggplot(df, aes(x = X, y = Y)) + geom_smooth(data = df, aes(x = X, y = Y), method = &quot;lm&quot;, formula = y ~ x, color = &quot;red&quot;, se = FALSE) + geom_point() result1 &lt;- lm( Y ~ X, data = df) coef(result1) ## (Intercept) X ## 1.119243 2.043244 Now generate the normally distributed residuals with mean zero and variance equal to 5, 4, 3, 2, 1/2, 1/3, 1/4, and 1/5. Use each residual to create a new \\(Y\\). Then, keeping the same \\(X\\) unchanged, regress all the resulting \\(Y\\)s on \\(X\\). Present all the resulting \\(R^2\\) in a new dataframe to then graph the relation between the variance of the residual and the \\(R^2\\). What lesson do you take out of this graph? set.seed(123) # For reproducibility df$ep5 &lt;- rnorm(1000, mean = 0, sd = 5) df$ep4 &lt;- rnorm(1000, mean = 0, sd = 4) df$ep3 &lt;- rnorm(1000, mean = 0, sd = 3) df$ep2 &lt;- rnorm(1000, mean = 0, sd = 2) df$ep1_2 &lt;- rnorm(1000, mean = 0, sd = 1/2) df$ep1_3 &lt;- rnorm(1000, mean = 0, sd = 1/3) df$ep1_4 &lt;- rnorm(1000, mean = 0, sd = 1/4) df$ep1_5 &lt;- rnorm(1000, mean = 0, sd = 1/5) df$Y5 &lt;- 1+2*df$X+df$ep5 df$Y4 &lt;- 1+2*df$X+df$ep4 df$Y3 &lt;- 1+2*df$X+df$ep3 df$Y2 &lt;- 1+2*df$X+df$ep2 df$Y_2 &lt;- 1+2*df$X+df$ep1_2 df$Y_3 &lt;- 1+2*df$X+df$ep1_3 df$Y_4 &lt;- 1+2*df$X+df$ep1_4 df$Y_5 &lt;- 1+2*df$X+df$ep1_5 result5 &lt;- lm( Y5 ~ X, data = df) result4 &lt;- lm( Y4 ~ X, data = df) result3 &lt;- lm( Y3 ~ X, data = df) result2 &lt;- lm( Y2 ~ X, data = df) result_2 &lt;- lm( Y_2 ~ X, data = df) result_3 &lt;- lm( Y_3 ~ X, data = df) result_4 &lt;- lm( Y_4 ~ X, data = df) result_5 &lt;- lm( Y_5 ~ X, data = df) df_rsqrd &lt;- data.frame( SD = c(5, 4, 3, 2, 1, 1/2, 1/3, 1/4, 1/5), R2 = c(summary(result5)$r.squared, summary(result4)$r.squared, summary(result3)$r.squared, summary(result2)$r.squared, summary(result1)$r.squared, summary(result_2)$r.squared, summary(result_3)$r.squared, summary(result_4)$r.squared, summary(result_5)$r.squared) ) ggplot(df_rsqrd, aes(x = SD, y = R2)) + geom_smooth(data = df_rsqrd, aes(x = SD, y = R2), method = &quot;lm&quot;, formula = y ~ x, color = &quot;red&quot;, se = FALSE) + geom_point() As shown in this entry, the bigger the sum of squared residuals is as a share of the regressand’s variance, the less capable out model to capture the relation between our regressand and regressor. Thus, as the graph shows, the smaller variance of the residual, the higher the \\(R^2\\). In other words, the more dispersed observations are around each section of the regression line, the lower the goodness of fit our model has to describe our data. In order to see the logic behind the \\(R^2\\), first estimate the linear regression of LeftToRight on Age. Generate the variable predicted_LeftToRight with the generated parameters using predict() from the point 3 of the previous section (i.e., the one using real data).Generate the graph predicted_LeftToRight vs Age. Report the regression here. result &lt;- lm(LeftToRight ~ age, data = selects19) selects19$predicted_LeftToRight &lt;- predict(result) coeffs1 &lt;- coef(result) rsq_sd1 &lt;- summary(result)$r.squared cat(&#39;The R2 is: &#39;, rsq_sd1, &#39;\\n\\nIntercept:&#39;,unname(coeffs1[1]), &#39;\\n\\nSlope:&#39;, unname(coeffs1[2])) ## The R2 is: 0.0161515 ## ## Intercept: 4.122055 ## ## Slope: 0.01948902 Report the graph here. # Calculate means for Y and X mean_LeftRight &lt;- mean(selects19$LeftToRight, na.rm = TRUE) mean_age &lt;- mean(selects19$age, na.rm = TRUE) # Create a scatter plot ggplot(selects19, aes(x = age, y = predicted_LeftToRight)) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;red&quot;) + # Adding the regression line geom_hline(yintercept = mean_LeftRight, linetype = &quot;dashed&quot;, color = &quot;green&quot;) + # Horizontal line at the mean of y geom_vline(xintercept = mean_age, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + # Vertical line at the mean of x scale_y_continuous( limits = c(0, 10), breaks = seq(0, 10, by = 1) ) + theme_minimal() + theme( plot.title = element_text(hjust = 0.5), # Center the plot title panel.grid.major.y = element_line(color = &quot;grey80&quot;, size = 0.5), # Style for major Y grid lines panel.grid.minor.y = element_blank() # Remove minor Y grid lines ) + labs( x = &quot;Age (years)&quot;, y = &quot;Political Preferences (left to right)&quot;, title = &quot;Scatter plot Age vs. Political Preferences (determ. sim.)&quot; ) Now generate the normally distributed residuals with mean zero and the same variance as the residuals of the model from the previous point. # Calculate the variance of the residuals selects19$residual &lt;- resid(result) residual_sd &lt;- sqrt(var(resid(result))) # Set the seed for reproducibility set.seed(0) selects19$NormResiduals1 &lt;- rnorm(nrow(selects19), mean = 0, sd = residual_sd) Simulate new data using both the deterministic (i.e., the the parameters estimated immediately above) and stochastic component (i.e., the residuals form the previous point). Graph the simulated data. selects19$Sim_LeftToRight_Det_Stoch &lt;- selects19$predicted_LeftToRight + selects19$NormResiduals1 #Means for Y and X mean_LeftRight &lt;- mean(selects19$Sim_LeftToRight_Det_Stoch, na.rm = TRUE) mean_age &lt;- mean(selects19$age, na.rm = TRUE) # Create a scatter plot ggplot(selects19, aes(x = age, y = Sim_LeftToRight_Det_Stoch)) + geom_point(size = 0) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;red&quot;) + # Adding the regression line geom_hline(yintercept = mean_LeftRight, linetype = &quot;dashed&quot;, color = &quot;green&quot;) + # Horizontal line at the mean of y geom_vline(xintercept = mean_age, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + # Vertical line at the mean of x theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + scale_y_continuous( limits = c(0, 10), breaks = seq(0, 10, by = 1)) + theme_minimal() + theme( plot.title = element_text(hjust = 0.5), # Center the plot title panel.grid.major.y = element_line(color = &quot;grey80&quot;, size = 0.5), # Style for major Y grid lines panel.grid.minor.y = element_blank() # Remove minor Y grid lines ) + labs( x = &quot;Age (years)&quot;, y = &quot;Simulation: Political Preferences (left to right)&quot;, title = &quot;Scatter plot Age vs. Political Preferences \\n(determ.&amp; stoch. sim.)&quot; ) Now, estimate the regression between the real age and simulated Sim_LeftToRight_Det_Stoch. Once you do that, generate the \\(R^2\\) for the model. result &lt;- lm(LeftToRight ~ age, data = selects19) rsq_sd1 &lt;- summary(result)$r.squared cat(&#39;The R2 is: &#39;, rsq_sd1) ## The R2 is: 0.0161515 Repeat the previous three points but use half the standard deviation of the model of point 1. Is the \\(R^2\\) lower now? Why? See this entry to understand the intuition: pay special attention to the graph with the red and blue squares. Generate the residuals here. # Set the seed for reproducibility set.seed(0) selects19$NormResiduals10 &lt;- rnorm(nrow(selects19), mean = 0, sd = residual_sd*0.1) Simulate new data and graph it here. selects19$Sim_LeftToRight_Det_Stoch_sd10 &lt;- selects19$predicted_LeftToRight + selects19$NormResiduals10 #Means for Y and X mean_LeftRight &lt;- mean(selects19$Sim_LeftToRight_Det_Stoch_sd10, na.rm = TRUE) mean_age &lt;- mean(selects19$age, na.rm = TRUE) # Create a scatter plot ggplot(selects19, aes(x = age, y = Sim_LeftToRight_Det_Stoch_sd10)) + geom_point(size = 0) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;red&quot;) + # Adding the regression line geom_hline(yintercept = mean_LeftRight, linetype = &quot;dashed&quot;, color = &quot;green&quot;) + # Horizontal line at the mean of y geom_vline(xintercept = mean_age, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + # Vertical line at the mean of x scale_y_continuous( limits = c(0, 10), breaks = seq(0, 10, by = 1)) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Age (years)&quot;, y = &quot;Simulated data (half sd): Political Preferences (left to right)&quot;, title = &quot;Scatter plot Age vs. Political Preferences&quot; ) Report the new \\(R^2\\) here: result &lt;- lm(Sim_LeftToRight_Det_Stoch_sd10 ~ age, data = selects19) rsq_sd10 &lt;- summary(result)$r.squared cat(&#39;The R2 is: &#39;, rsq_sd10) ## The R2 is: 0.625503 Repeat the previous point but use a hundredth the standard deviation of the model of point 1. Is the \\(R^2\\) lower now? Why? Generate the residuals here. # Set the seed for reproducibility set.seed(0) selects19$NormResiduals100 &lt;- rnorm(nrow(selects19), mean = 0, sd = residual_sd*0.01) Simulate new data and graph it here. selects19$Sim_LeftToRight_Det_Stoch_hsd &lt;- selects19$predicted_LeftToRight + selects19$NormResiduals100 #Means for Y and X mean_LeftRight &lt;- mean(selects19$Sim_LeftToRight_Det_Stoch_hsd, na.rm = TRUE) mean_age &lt;- mean(selects19$age, na.rm = TRUE) # Create a scatter plot ggplot(selects19, aes(x = age, y = Sim_LeftToRight_Det_Stoch_hsd)) + geom_point(size = 0) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;red&quot;) + # Adding the regression line geom_hline(yintercept = mean_LeftRight, linetype = &quot;dashed&quot;, color = &quot;green&quot;) + # Horizontal line at the mean of y geom_vline(xintercept = mean_age, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + # Vertical line at the mean of x scale_y_continuous( limits = c(0, 10), breaks = seq(0, 10, by = 1)) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Age (years)&quot;, y = &quot;Simulated data (half sd): Political Preferences (left to right)&quot;, title = &quot;Scatter plot Age vs. Political Preferences&quot; ) Report the new \\(R^2\\) here: result &lt;- lm(Sim_LeftToRight_Det_Stoch_hsd ~ age, data = selects19) rsq_sd100 &lt;- summary(result)$r.squared cat(&#39;The R2 is: &#39;, rsq_sd100) ## The R2 is: 0.9939772 Present in a table the \\(R^2\\) for each value of the standard deviation. table &lt;- data.frame( SD = c(1, 0.1, 0.01), R2 = c(rsq_sd1, rsq_sd10, rsq_sd100) # Replace these with the actual R-squared values ) print(table) ## SD R2 ## 1 1.00 0.0161515 ## 2 0.10 0.6255030 ## 3 0.01 0.9939772 Graph \\(R^2\\) agains the standard deviation fraction. Fit a linear regression line. ggplot(table, aes(x = SD, y = R2)) + geom_point(size = 0) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;green&quot;)+ theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Fraction of standard deviation&quot;, y = &quot;R2&quot;, title = &quot;Relation between R2 and standard deviation of residuals&quot; ) 11. [Optional] Graph \\(R^2\\) against the standard deviation fraction. Fit a regression line with a polynomial of degree 2. The regression line fits the data better. Is the polynomial regression more informative than the linear regression? Why? What lesson does this graph give you in terms of the importance of understanding the theoretical relations between variables beyond what an empirical approach could suggest? What does it teach you regarding the tension between under- and over-fitting data? ggplot(table, aes(x = SD, y = R2)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, 2), se = FALSE, color=&#39;green&#39;) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Fraction of standard deviation&quot;, y = &quot;R2&quot;, title = &quot;Relation between R2 and standard deviation of residuals&quot; ) 12. Print the parameters generated in the point 1. Generate the same predictions as in point 1 and 2 but this time multiply the slope’s parameter by 5, while keeping the intercept unchanged. Call the resulting predictions predicted_LeftToRight_AdjSlope Generate a graph with the regression line for predicted_LeftToRight_AdjSlope vs Age and compare it with the graph predicted_LeftToRight vs Age generated in point 1. Print here result &lt;- lm(LeftToRight ~ age, data = selects19) # summary(result) coeffs &lt;- coef(result) cat(&#39;Intercept:&#39;,unname(coeffs[1]), &#39;\\n\\nSlope:&#39;, unname(coeffs[2])) ## Intercept: 4.122055 ## ## Slope: 0.01948902 Predict with the adjusted slope here. selects19$predicted_LeftToRight_AdjSlope &lt;- unname(coeffs[1])+(unname(coeffs[2])*5)*selects19$age Generate the graph here. #Means for Y and X mean_LeftRight &lt;- mean(selects19$predicted_LeftToRight_AdjSlope, na.rm = TRUE) mean_age &lt;- mean(selects19$age, na.rm = TRUE) # Create a scatter plot ggplot(selects19, aes(x = age, y = predicted_LeftToRight_AdjSlope)) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;red&quot;) + # Adding the regression line geom_hline(yintercept = mean_LeftRight, linetype = &quot;dashed&quot;, color = &quot;green&quot;) + # Horizontal line at the mean of y geom_vline(xintercept = mean_age, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + # Vertical line at the mean of x theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + scale_y_continuous( limits = c(0, 15), breaks = seq(0, 20, by = 1)) + theme_minimal() + theme( plot.title = element_text(hjust = 0.5), # Center the plot title panel.grid.major.y = element_line(color = &quot;grey80&quot;, size = 0.5), # Style for major Y grid lines panel.grid.minor.y = element_blank() # Remove minor Y grid lines ) + labs( x = &quot;Age (years)&quot;, y = &quot;Political Preferences (left to right)&quot;, title = &quot;Scatter plot Age vs. Political Preferences (adj. slope)&quot; ) 13. Add to the deterministic simulation the same stochastic component generated in point 2. Call that variable predicted_LeftToRight_AdjSlope_Stoch. Generate the graph predicted_LeftToRight_AdjSlope_Stoch vs age with a linear regression line. Generate the \\(R^2\\) and compare it to the one generated in point 4. Why is the new \\(R^2\\) higher than the one in point 4? Report the simulation and the graph here. selects19$predicted_LeftToRight_AdjSlope_Stoch &lt;- selects19$predicted_LeftToRight_AdjSlope + selects19$NormResiduals1 #Means for Y and X mean_LeftRight &lt;- mean(selects19$predicted_LeftToRight_AdjSlope_Stoch, na.rm = TRUE) mean_age &lt;- mean(selects19$age, na.rm = TRUE) # Create a scatter plot ggplot(selects19, aes(x = age, y = predicted_LeftToRight_AdjSlope_Stoch)) + geom_point(size = 0) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;red&quot;) + # Adding the regression line geom_hline(yintercept = mean_LeftRight, linetype = &quot;dashed&quot;, color = &quot;green&quot;) + # Horizontal line at the mean of y geom_vline(xintercept = mean_age, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + # Vertical line at the mean of x scale_y_continuous( limits = c(0, 20), breaks = seq(0, 20, by = 1)) + theme_minimal() + theme( plot.title = element_text(hjust = 0.5), # Center the plot title panel.grid.major.y = element_line(color = &quot;grey80&quot;, size = 0.5), # Style for major Y grid lines panel.grid.minor.y = element_blank() # Remove minor Y grid lines ) + labs( x = &quot;Age (years)&quot;, y = &quot;Simulation: Political Preferences (left to right)&quot;, title = &quot;Scatter plot Age vs. Political Preferences \\n(determ.&amp; stoch. sim. with adjusted slope)&quot; ) Report the new \\(R^2\\) here: result &lt;- lm(predicted_LeftToRight_AdjSlope_Stoch ~ age, data = selects19) rsq_sd1_AdjSlope &lt;- summary(result)$r.squared cat(&#39;The R2 is: &#39;, rsq_sd1_AdjSlope) ## The R2 is: 0.2973214 Report why is the new \\(R^2\\) higher than the one in point 4? print(&#39;The reason the R2 increased is that, while the distance from each observation to the regression line remained unchainged (i.e., the residual was kept unchanged), by increasing the slope of the generating process, the average distance between each observation and the average observation increased (i.e., Y\\&#39;s variance increased). See the graph with the red and blue squares mentioned in point 5.&#39;) ## [1] &quot;The reason the R2 increased is that, while the distance from each observation to the regression line remained unchainged (i.e., the residual was kept unchanged), by increasing the slope of the generating process, the average distance between each observation and the average observation increased (i.e., Y&#39;s variance increased). See the graph with the red and blue squares mentioned in point 5.&quot; 333333333333 Print the parameters generated in the point 1. Generate the same predictions as in point 1 and 2 but this time increase in two units the intercept’s parameter (not the slope’s parameter as in the point 10), while keeping the slope unchanged. Call the resulting predictions predicted_LeftToRight_AdjSlope Generate a graph with the regression line for predicted_LeftToRight_AdjSlope vs Age and compare it with the graph predicted_LeftToRight vs Age generated in point 1. Print here result &lt;- lm(predicted_LeftToRight_AdjSlope_Stoch ~ age, data = selects19) # summary(result) coeffs12 &lt;- coef(result) cat(&#39;Intercept:&#39;,unname(coeffs12[1]), &#39;\\n\\nSlope:&#39;, unname(coeffs12[2])) ## Intercept: 4.071965 ## ## Slope: 0.09874777 Predict with the adjusted slope here. selects19$predicted_LeftToRight_AdjIntecept &lt;- unname(coeffs[1]+2)+(unname(coeffs[2]))*selects19$age Generate the graph here. #Means for Y and X mean_LeftRight &lt;- mean(selects19$predicted_LeftToRight_AdjIntecept, na.rm = TRUE) mean_age &lt;- mean(selects19$age, na.rm = TRUE) # Create a scatter plot ggplot(selects19, aes(x = age, y = predicted_LeftToRight_AdjIntecept)) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;red&quot;) + # Adding the regression line geom_hline(yintercept = mean_LeftRight, linetype = &quot;dashed&quot;, color = &quot;green&quot;) + # Horizontal line at the mean of y geom_vline(xintercept = mean_age, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + # Vertical line at the mean of x theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + scale_y_continuous( limits = c(0, 15), breaks = seq(0, 15, by = 1)) + theme_minimal() + theme( plot.title = element_text(hjust = 0.5), # Center the plot title panel.grid.major.y = element_line(color = &quot;grey80&quot;, size = 0.5), # Style for major Y grid lines panel.grid.minor.y = element_blank() # Remove minor Y grid lines ) + labs( x = &quot;Age (years)&quot;, y = &quot;Political Preferences (left to right)&quot;, title = &quot;Scatter plot Age vs. Political Preferences (adj. slope)&quot; ) 15. Add to the deterministic simulation the same stochastic component generated in point 2. Call that variable predicted_LeftToRight_AdjIntercep_Stoch. Generate the graph predicted_LeftToRight_AdjIntecept vs age with a linear regression line. Generate the \\(R^2\\) and compare it to the one generated in point 4. Why is the new \\(R^2\\) higher than the one in point 4? Report the simulation and the graph here. selects19$predicted_LeftToRight_AdjIntercept_Stoch &lt;- selects19$predicted_LeftToRight_AdjIntecept + selects19$residual #Means for Y and X mean_LeftRight &lt;- mean(selects19$predicted_LeftToRight_AdjIntercept_Stoch, na.rm = TRUE) mean_age &lt;- mean(selects19$age, na.rm = TRUE) # Create a scatter plot ggplot(selects19, aes(x = age, y = predicted_LeftToRight_AdjIntercept_Stoch)) + geom_point(size = 0) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;red&quot;) + # Adding the regression line geom_hline(yintercept = mean_LeftRight, linetype = &quot;dashed&quot;, color = &quot;green&quot;) + # Horizontal line at the mean of y geom_vline(xintercept = mean_age, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + # Vertical line at the mean of x scale_y_continuous( limits = c(0, 15), breaks = seq(0, 15, by = 1)) + theme_minimal() + theme( plot.title = element_text(hjust = 0.5), # Center the plot title panel.grid.major.y = element_line(color = &quot;grey80&quot;, size = 0.5), # Style for major Y grid lines panel.grid.minor.y = element_blank() # Remove minor Y grid lines ) + labs( x = &quot;Age (years)&quot;, y = &quot;Simulation: Political Preferences (left to right)&quot;, title = &quot;Scatter plot Age vs. Political Preferences \\n(determ.&amp; stoch. sim. with adjusted intercept)&quot; ) Regress the predicted_LeftToRight_AdjIntercept_Stoch on age result &lt;- lm(predicted_LeftToRight_AdjIntercept_Stoch ~ age, data = selects19) rsq_sd1_AdjIntercept &lt;- summary(result)$r.squared coeffs13 &lt;- coef(result) cat(&#39;Intercept:&#39;,unname(coeffs13[1]), &#39;\\n\\nSlope:&#39;, unname(coeffs13[2])) ## Intercept: 6.122055 ## ## Slope: 0.01948902 Report the \\(R^2\\) generated in points 2, 11, and 13 here: tableSD1 &lt;- data.frame( LookR2 = c(&#39;Point 2&#39;, &#39;Point 12&#39;, &#39;Point 13&#39;), Model = c(&#39;Unchanged&#39;, &#39;P2 + (slope*5)&#39;, &#39;P2+(intercept+2)&#39;), R2 = c(rsq_sd1, rsq_sd1_AdjSlope, rsq_sd1_AdjIntercept), LookParams = c(&#39;Point 1&#39;, &#39;Point 12&#39;, &#39;Point 13&#39;), Intercept = c(unname(coeffs1[1]), unname(coeffs12[1]), unname(coeffs13[1])), Slope = c(unname(coeffs1[2]), unname(coeffs12[2]), unname(coeffs13[2])) ) print(tableSD1) ## LookR2 Model R2 LookParams Intercept Slope ## 1 Point 2 Unchanged 0.0161515 Point 1 4.122055 0.01948902 ## 2 Point 12 P2 + (slope*5) 0.2973214 Point 12 4.071965 0.09874777 ## 3 Point 13 P2+(intercept+2) 0.0161515 Point 13 6.122055 0.01948902 Comparing the \\(R^2\\) and the parameters across the models of points 1, 2, and 3 answer the following questions: What effect does increasing the slope of the data generating process (while keeping the residuals and intercept unchanged) have on the \\(R^2\\)? print(&#39;The R2 increases as seen in point 13&#39;) ## [1] &quot;The R2 increases as seen in point 13&quot; What effect does increasing the intercept of the data generating process (while keeping the residuals and slope unchanged) have on the \\(R^2\\)? print(&#39;The R2 stays unchanged as the distance from each observation to the regression line remained unchainged (i.e., the residual was kept unchanged) as well as the average distance between each observation and the average observation remained unchainged (i.e., Y\\&#39;s variance was kept unchanged)&#39;) ## [1] &quot;The R2 stays unchanged as the distance from each observation to the regression line remained unchainged (i.e., the residual was kept unchanged) as well as the average distance between each observation and the average observation remained unchainged (i.e., Y&#39;s variance was kept unchanged)&quot; What effect does increasing the slope of the data generating process (while keeping the residuals and intercept unchanged) have on the intercept and slope? print(&#39;It increases the estimated slope and decreases the intercept.&#39;) ## [1] &quot;It increases the estimated slope and decreases the intercept.&quot; What effect does increasing the intercept of the data generating process (while keeping the residuals and slope unchanged) have on the intercept and slope? print(&#39;It keeps the estimated slope unchanged and the estimated intercept is the same as before the adjustment but increases by the same value that the whole data generating process was modified.&#39;) ## [1] &quot;It keeps the estimated slope unchanged and the estimated intercept is the same as before the adjustment but increases by the same value that the whole data generating process was modified.&quot; "],["week-5.html", "Chapter 5 Week 5 5.1 Exercise 5.2 Solution", " Chapter 5 Week 5 5.1 Exercise 5.2 Solution "],["week-6.html", "Chapter 6 Week 6 6.1 Exercise 6.2 Solution", " Chapter 6 Week 6 6.1 Exercise 6.2 Solution "],["week-7.html", "Chapter 7 Week 7 7.1 Exercise 7.2 Solution", " Chapter 7 Week 7 7.1 Exercise 7.2 Solution "],["week-8-causality-iii-observational-data.html", "Chapter 8 Week 8: Causality III (Observational Data) 8.1 Aims 8.2 Exercise", " Chapter 8 Week 8: Causality III (Observational Data) 8.1 Aims Using simulated data to instantiate the logic behind, diff-in-diff and panel data (i.e., within, between, and twoway fixed effects) methods. Using real data to estimate a causal effect using a two’way fixed effects model. 8.2 Exercise 8.2.1 Simulated data The following data is NOT real and is inspired by Ben Lambert’s video on fixed effects (see here). It’s purpose is simply to illustrate the logic og the fixed effects models. data &lt;- list( City = c(&#39;NYC&#39;, &#39;NYC&#39;, &#39;NYC&#39;, &#39;Boston&#39;, &#39;Boston&#39;, &#39;Boston&#39;, &#39;Amherst&#39;, &#39;Amherst&#39;, &#39;Amherst&#39;), Time = c(1, 2, 3, 1, 2, 3, 1, 2, 3), Unemployment = c(2, 3, 3, 4, 5, 5, 6, 7, 8), Crime = c(5, 6, 9, 3, 4, 7, 1, 2, 3), Treatment = c(0, 0, 1, 0, 0, 1, 0, 0, 0) ) df &lt;- data.frame(data) # Overall averages df &lt;- df %&gt;% mutate(Y_OverallAverage = mean(Crime), X_OverallAverage = mean(Unemployment)) # Within averages df &lt;- df %&gt;% group_by(City) %&gt;% mutate(Y_WithinAverage = mean(Crime), X_WithinAverage = mean(Unemployment)) # Between averages df &lt;- df %&gt;% group_by(Time) %&gt;% mutate(Y_BetweenAverage = mean(Crime), X_BetweenAverage = mean(Unemployment)) df&lt;- df %&gt;% mutate( Y_demeaned_within = Crime-Y_WithinAverage, Y_demeaned_between = Crime-Y_BetweenAverage, Y_double_demeaned = Crime-Y_WithinAverage-Y_BetweenAverage+Y_OverallAverage, X_demeaned_within = Unemployment-X_WithinAverage, X_demeaned_between = Unemployment-X_BetweenAverage, X_double_demeaned = Unemployment-X_WithinAverage-X_BetweenAverage+X_OverallAverage ) First the basic graph ggplot(df, aes(x = Unemployment, y = Crime)) + geom_point(size = 5) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;gray&quot;) + # Adding the regression line scale_y_continuous( limits = c(0, 10), breaks = seq(0, 10, by = 1)) + theme_minimal()+ theme(plot.title = element_text(hjust = 0.5)) + labs(title = &quot;Unemployment vs Crime&quot;, x = &quot;Unemployment&quot;, y = &quot;Crime&quot; ) Now use shapes for distinguishing cities. ggplot(df, aes(x = Unemployment, y = Crime)) + geom_point(aes(shape = factor(City)), size = 5) + # Apply shape by City only to points geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;gray&quot;) + # Regression line for all data scale_y_continuous( limits = c(0, 10), breaks = seq(0, 10, by = 1)) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs(title = &quot;Unemployment vs Crime by City and Time&quot;, x = &quot;Unemployment&quot;, y = &quot;Crime&quot;, shape = &quot;City&quot;) Now also use color for distinguishing also periods. ggplot(df, aes(x = Unemployment, y = Crime)) + geom_point(aes(shape = factor(City), color = factor(Time)), size = 5) + # Apply shape by City only to points scale_color_manual(values = c(&quot;gold&quot;, &quot;orange&quot;, &quot;red&quot;)) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;gray&quot;) + # Regression line for all data scale_y_continuous( limits = c(0, 10), breaks = seq(0, 10, by = 1)) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs(title = &quot;Unemployment vs Crime by City and Time&quot;, x = &quot;Unemployment&quot;, y = &quot;Crime&quot;, color = &quot;Time&quot;, shape = &quot;City&quot;) All previous regressions are counter intuitive, why?. Now restrict the regression to each city. What does this suggest regarding the ability of linear regression to capture relations between our variables of interest this scenario? Can we use a regression regardless of how we frame it? Is theory important for guiding how the regression is used? If so, in this case, how? ggplot(df, aes(x = Unemployment, y = Crime, color = factor(Time), shape = factor(City))) + geom_point(size = 5) + scale_color_manual(values = c(&quot;gold&quot;, &quot;orange&quot;, &quot;red&quot;)) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;gray&quot;) + # Adding the regression line scale_y_continuous( limits = c(0, 10), breaks = seq(0, 10, by = 1)) + labs(title = &quot;Unemployment vs Crime by City and Time&quot;, x = &quot;Unemployment&quot;, y = &quot;Crime&quot;, color = &quot;Time&quot;, shape = &quot;City&quot;) + theme_minimal() Now, having seen how regressions are theory dependent for defining how they are framed, let’s see how fixed effects work. Assume that the data generating process behind the relation between unemployment and crime is given by the following formula. \\[Y_{it}=\\alpha_{i}+\\beta_{t}+\\gamma_{it}X_{it}+\\epsilon_{it}\\] With \\[t=1, ..., T\\] and \\[i=1, ..., N\\] Where \\(Y_{it}\\) is crime for city \\(i\\) at time \\(t\\), \\(\\alpha_{i}\\) is a time invariant unobservable (or unobserved) factor influencing the levels of crime in city \\(i\\) (i.e., it’s influence is constant across all periods). \\(\\beta_{t}\\) is a city invariant unobservable (or unobserved) factor influencing the levels of crime at period \\(t\\) (i.e., it’s influence is constant across all cities). \\(\\gamma_{it}\\) is the marginal effect of \\(X_{it}\\), the unemployment level at city \\(i\\) in period \\(t\\). Finally, \\(\\epsilon_{it}\\) is the residual for city \\(i\\) in period \\(t\\) Thus, in order to get rid of the time invariant factor, we can implement the following transformation. \\(\\bar{X}_{i.} = \\frac{1}{T}*sum_{t=1}^{T} X_{it}.\\) perhaps showing only the within case is enough? I’m considering to use this paper, particularly page 8. Plus vertical line for within mean values. X_WithinAverage_NYC = df$X_WithinAverage[df$City==&#39;NYC&#39;][1] X_WithinAverage_Boston = df$X_WithinAverage[df$City==&#39;Boston&#39;][1] X_WithinAverage_Amherst = df$X_WithinAverage[df$City==&#39;Amherst&#39;][1] ggplot(df, aes(x = Unemployment, y = Crime, color = factor(Time), shape = factor(City))) + geom_point(size = 5) + scale_color_manual(values = c(&quot;gold&quot;, &quot;orange&quot;, &quot;red&quot;)) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;gray&quot;) + # Adding the regression line geom_vline(xintercept = X_WithinAverage_NYC, linetype = &quot;dashed&quot;, color = &quot;lightgray&quot;) + # Vertical line at the mean of geom_vline(xintercept = X_WithinAverage_Boston, linetype = &quot;dashed&quot;, color = &quot;lightgray&quot;) + # Vertical line at the mean of geom_vline(xintercept = X_WithinAverage_Amherst, linetype = &quot;dashed&quot;, color = &quot;lightgray&quot;) + # Vertical line at the mean of scale_y_continuous( limits = c(0, 10), breaks = seq(0, 10, by = 1)) + labs(title = &quot;Unemployment vs Crime by City and Time&quot;, x = &quot;Unemployment&quot;, y = &quot;Crime&quot;, color = &quot;Time&quot;, shape = &quot;City&quot;) + theme_minimal() Demeaned within unemployment. ggplot(df, aes(x = X_demeaned_within, y = Crime, color = factor(Time), shape = factor(City))) + geom_point(size = 5) + scale_color_manual(values = c(&quot;gold&quot;, &quot;orange&quot;, &quot;red&quot;)) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;gray&quot;) + # Adding the regression line scale_y_continuous( limits = c(0, 10), breaks = seq(0, 10, by = 1)) + labs(title = &quot;Demeaned within unemployment vs Crime by City and Time&quot;, x = &quot;Demeaned unemployment&quot;, y = &quot;Crime&quot;, color = &quot;Time&quot;, shape = &quot;City&quot;) + theme_minimal() Also demeaned within crime. ggplot(df, aes(x = X_demeaned_within, y = Y_demeaned_within)) + geom_point(aes(shape = factor(City), color = factor(Time)), size = 5) + scale_color_manual(values = c(&quot;gold&quot;, &quot;orange&quot;, &quot;red&quot;)) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;gray&quot;) + # Adding the regression line labs(title = &quot;Demeaned unemployment vs Demeaned crime by City and Time (within )&quot;, x = &quot;Demeaned unemployment&quot;, y = &quot;Demeaned crime&quot;, color = &quot;Time&quot;, shape = &quot;City&quot;) + theme_minimal() Unemployment over time with horizontal line for between mean values. X_BetweenAverage_T1 = df$X_BetweenAverage[df$Time==&#39;1&#39;][1] X_BetweenAverage_T2 = df$X_BetweenAverage[df$Time==&#39;2&#39;][1] X_BetweenAverage_T3 = df$X_BetweenAverage[df$Time==&#39;3&#39;][1] ggplot(df, aes(x = Time, y = Unemployment, color = factor(Time), shape = factor(City))) + geom_point(size = 5) + scale_color_manual(values = c(&quot;gold&quot;, &quot;orange&quot;, &quot;red&quot;)) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;gray&quot;) + # Adding the regression line geom_hline(yintercept = X_BetweenAverage_T1, linetype = &quot;dashed&quot;, color = &quot;gold&quot;) + # Vertical line at the mean of geom_hline(yintercept = X_BetweenAverage_T2, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + # Vertical line at the mean of geom_hline(yintercept = X_BetweenAverage_T3, linetype = &quot;dashed&quot;, color = &quot;red&quot;) + # Vertical line at the mean of labs(title = &quot;Time vs Unemployment by City and Time&quot;, x = &quot;Time&quot;, y = &quot;Unemployment&quot;, color = &quot;Time&quot;, shape = &quot;City&quot;) + theme_minimal() Crime over time with horizontal line for between mean values. Y_BetweenAverage_T1 = df$Y_BetweenAverage[df$Time==&#39;1&#39;][1] Y_BetweenAverage_T2 = df$Y_BetweenAverage[df$Time==&#39;2&#39;][1] Y_BetweenAverage_T3 = df$Y_BetweenAverage[df$Time==&#39;3&#39;][1] ggplot(df, aes(x = Time, y = Crime, color = factor(Time), shape = factor(City))) + geom_point(size = 5) + scale_color_manual(values = c(&quot;gold&quot;, &quot;orange&quot;, &quot;red&quot;)) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;gray&quot;) + # Adding the regression line geom_hline(yintercept = Y_BetweenAverage_T1, linetype = &quot;dotted&quot;, color = &quot;lightgray&quot;) + # Vertical line at the mean of geom_hline(yintercept = Y_BetweenAverage_T2, linetype = &quot;twodash&quot;, color = &quot;lightgray&quot;) + # Vertical line at the mean of geom_hline(yintercept = Y_BetweenAverage_T3, linetype = &quot;dashed&quot;, color = &quot;lightgray&quot;) + # Vertical line at the mean of labs(title = &quot;Time vs Crime by City and Time&quot;, x = &quot;Time&quot;, y = &quot;Crime&quot;, color = &quot;Time&quot;, shape = &quot;City&quot;) + theme_minimal() Demeaned unemployment over time with horizontal line for between mean values. ggplot(df, aes(x = Time, y = X_demeaned_between, color = factor(Time), shape = factor(City))) + geom_point(size = 5) + scale_color_manual(values = c(&quot;gold&quot;, &quot;orange&quot;, &quot;red&quot;)) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;gray&quot;) + # Adding the regression line labs(title = &quot;Time vs demeaned between unemployment by City and Time&quot;, x = &quot;Time&quot;, y = &quot;Demeaned unemployment&quot;, color = &quot;Time&quot;, shape = &quot;City&quot;) + theme_minimal() Demeaned crime over time with horizontal line for between mean values. ggplot(df, aes(x = Time, y = Y_demeaned_between, color = factor(Time), shape = factor(City))) + geom_point(size = 5) + scale_color_manual(values = c(&quot;gold&quot;, &quot;orange&quot;, &quot;red&quot;)) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;gray&quot;) + # Adding the regression line # scale_y_continuous( # limits = c(0, 10), # breaks = seq(0, 10, by = 1)) + labs(title = &quot;Time vs demeaned between crime by City and Time&quot;, x = &quot;Time&quot;, y = &quot;Demeaned crime&quot;, color = &quot;Time&quot;, shape = &quot;City&quot;) + theme_minimal() Demeaned crime over time with horizontal line for between mean values. Very hard to interpret! ggplot(df, aes(x = X_demeaned_between, y = Y_demeaned_between)) + geom_point(aes(shape = factor(City), color = factor(Time)), size = 5) + # Apply shape by City only to points scale_color_manual(values = c(&quot;gold&quot;, &quot;orange&quot;, &quot;red&quot;)) + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;gray&quot;) + # Adding the regression line labs(title = &quot;Demeaned unemployment vs demeaned crime by City and Time (between)&quot;, x = &quot;Demeaned unemployment&quot;, y = &quot;Demeaned crime&quot;, color = &quot;Time&quot;, shape = &quot;City&quot;) + theme_minimal() Missing!! all demeaned (i.e., twoway fixed effects) 8.2.2 Real data df1 &lt;- swissdd::get_nationalvotes(geolevel = &quot;national&quot;, from_date=&quot;2000-03-12&quot;, to_date = &quot;2015-06-14&quot;) df1&lt;- df1 %&gt;% select(&quot;stimmbeteiligungInProzent&quot;,&quot;votedate&quot;) df1 &lt;- dplyr::rename(df1, Y_National_BetweenAverage = stimmbeteiligungInProzent, vote_date = votedate) ballot_days_final &lt;- readRDS(&quot;Data/replication/data/ballot_days_final.rds&quot;) ballot_days_final &lt;- inner_join(ballot_days_final, df1, by=&quot;vote_date&quot;) ballot_days_final &lt;- ballot_days_final %&gt;% group_by(muninr) %&gt;% mutate(Y_municipal_WithinAverage = mean(turnout, na.rm = TRUE), X_municipal_WithinAverage = mean(postage, na.rm = TRUE)) ballot_days_final &lt;- ballot_days_final %&gt;% mutate(Y_National_OverallAverage = mean(turnout, na.rm = TRUE), X_National_OverallAverage = mean(postage, na.rm = TRUE)) ballot_days_final &lt;- ballot_days_final %&gt;% group_by(vote_date) %&gt;% mutate(X_National_BetweenAverage = mean(postage, na.rm = TRUE)) ballot_days_final&lt;- ballot_days_final %&gt;% mutate( Y_demeaned_within = turnout-Y_municipal_WithinAverage, Y_demeaned_between = turnout-Y_National_BetweenAverage, Y_double_demeaned = turnout-Y_municipal_WithinAverage-Y_National_BetweenAverage+Y_National_OverallAverage, X_demeaned_within = postage-X_municipal_WithinAverage, X_demeaned_between = postage-X_National_BetweenAverage, X_double_demeaned = postage-X_municipal_WithinAverage-X_National_BetweenAverage+X_National_OverallAverage ) Results: One way fixed effects: within # One way fixed effects: within OW_FE_WI &lt;- lm(Y_demeaned_within ~ X_demeaned_within - 1, data = ballot_days_final) summary(OW_FE_WI) ## ## Call: ## lm(formula = Y_demeaned_within ~ X_demeaned_within - 1, data = ballot_days_final) ## ## Residuals: ## Min 1Q Median 3Q Max ## -37.03 -6.53 -0.03 6.67 53.07 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## X_demeaned_within 1.46 0.15 9.73 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.01 on 89319 degrees of freedom ## (1210 observations deleted due to missingness) ## Multiple R-squared: 0.001059, Adjusted R-squared: 0.001048 ## F-statistic: 94.67 on 1 and 89319 DF, p-value: &lt; 2.2e-16 Results: One way fixed effects: between # One way fixed effects: between OW_FE_BE &lt;- lm(Y_demeaned_between ~ X_demeaned_between - 1, data = ballot_days_final) summary(OW_FE_BE) ## ## Call: ## lm(formula = Y_demeaned_between ~ X_demeaned_between - 1, data = ballot_days_final) ## ## Residuals: ## Min 1Q Median 3Q Max ## -32.706 -6.728 -2.224 2.430 48.172 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## X_demeaned_between 1.3944 0.1132 12.32 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.553 on 89319 degrees of freedom ## (1210 observations deleted due to missingness) ## Multiple R-squared: 0.001696, Adjusted R-squared: 0.001685 ## F-statistic: 151.8 on 1 and 89319 DF, p-value: &lt; 2.2e-16 Results: Two way fixed effects: between # Two way fixed effects: between TW_FE &lt;- lm(Y_double_demeaned ~ X_double_demeaned - 1, data = ballot_days_final) summary(TW_FE) ## ## Call: ## lm(formula = Y_double_demeaned ~ X_double_demeaned - 1, data = ballot_days_final) ## ## Residuals: ## Min 1Q Median 3Q Max ## -32.706 -6.728 -2.224 2.430 48.172 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## X_double_demeaned 1.3944 0.1132 12.32 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.553 on 89319 degrees of freedom ## (1210 observations deleted due to missingness) ## Multiple R-squared: 0.001696, Adjusted R-squared: 0.001685 ## F-statistic: 151.8 on 1 and 89319 DF, p-value: &lt; 2.2e-16 Introduce here the estimation using plm "],["week-9.html", "Chapter 9 Week 9 9.1 Exercise 9.2 Solution", " Chapter 9 Week 9 9.1 Exercise 9.2 Solution "],["week-10.html", "Chapter 10 Week 10 10.1 Exercise 10.2 Solution", " Chapter 10 Week 10 10.1 Exercise 10.2 Solution "],["week-11.html", "Chapter 11 Week 11 11.1 Exercise 11.2 Solution", " Chapter 11 Week 11 11.1 Exercise 11.2 Solution "],["week-12.html", "Chapter 12 Week 12 12.1 Exercise 12.2 Solution", " Chapter 12 Week 12 12.1 Exercise 12.2 Solution "],["week-13.html", "Chapter 13 Week 13 13.1 Exercise 13.2 Solution", " Chapter 13 Week 13 13.1 Exercise 13.2 Solution "],["week-14.html", "Chapter 14 Week 14 14.1 Exercise 14.2 Solution", " Chapter 14 Week 14 14.1 Exercise 14.2 Solution "],["potentially-useful-exercises.html", "Chapter 15 Potentially useful exercises 15.1 Taken out from Week 3 15.2 Taken out from Week 4", " Chapter 15 Potentially useful exercises 15.1 Taken out from Week 3 Parabolic simulated residuals: CO2_land$parab_norm_resid&lt;- -0.01*(CO2_land$Sim_ShareAgric_norm-mean(CO2_land$Sim_ShareAgric_norm))^2+CO2_land$Sim_resid_norm CO2_land$parab_norm_resid&lt;- CO2_land$parab_norm_resid-mean(CO2_land$parab_norm_resid) ggplot(CO2_land, aes(x = Sim_ShareAgric_norm, y = parab_norm_resid)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;green&quot;) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Agricultural Area (%)&quot;, y = &quot;Error&quot;, title = &quot;Simulation: of parabolic error&quot; ) What does the behavior of the error tell you about the potential reliability of a regression without quadratic terms? Data generating process with parabolic residuals: CO2_land$Pred_TurnOut_parab_norm_resid &lt;- CO2_land$Pred_TurnOut_Determ + CO2_land$parab_norm_resid ggplot(CO2_land, aes(x = Sim_ShareAgric_norm, y = Pred_TurnOut_parab_norm_resid)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;purple&quot;) + geom_hline(yintercept = mean(CO2_land$Pred_TurnOut_parab_norm_resid), linetype = &quot;dashed&quot;, color = &quot;green&quot;) + geom_vline(xintercept = mean_AgricAreaPercent, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + scale_y_continuous(limits = c(25, max(CO2_land$Pred_TurnOut_Unif, na.rm = TRUE))) + # theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Agricultural Area (%)&quot;, y = &quot;Turn-out (%)&quot;, title = &quot;Simulation: deterministic and stochastic model (uniform error)&quot; ) Would using a quadratic term in the regression produce a better fit? Graph the regression line with a quadratic term. ggplot(CO2_land, aes(x = Sim_ShareAgric_norm, y = Pred_TurnOut_parab_norm_resid)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ x + I(x^2), se = FALSE, color = &quot;purple&quot;) + geom_hline(yintercept = mean(CO2_land$Pred_TurnOut_parab_norm_resid), linetype = &quot;dashed&quot;, color = &quot;green&quot;) + geom_vline(xintercept = mean_AgricAreaPercent, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + scale_y_continuous(limits = c(25, max(CO2_land$Pred_TurnOut_Unif, na.rm = TRUE))) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5)) + labs( x = &quot;Agricultural Area (%)&quot;, y = &quot;Turn-out (%)&quot;, title = &quot;Simulation: deterministic and stochastic model (uniform error)&quot; ) 15.2 Taken out from Week 4 # ## Interactive Line Plot # # *&lt;span style=&quot;color: red;&quot;&gt;Should we exploit this here or somewhere else? I can make it work in R, but not in the book yet. If needed, I can check it out.&lt;/span&gt;.* # # Example data frame # data &lt;- data.frame( # x = rnorm(100), # y = rnorm(100) # ) # # # Shiny server function # server &lt;- function(input, output) { # output$scatterPlot &lt;- renderPlot({ # ggplot(data, aes(x = x, y = y)) + # geom_point(size = 0) + # geom_abline(intercept = input$intercept, slope = input$slope, color = &quot;red&quot;, size = 1) # }) # } # # # Example data frame # data &lt;- data.frame( # x = rnorm(100), # y = rnorm(100) # ) # # # Shiny server function # server &lt;- function(input, output) { # output$scatterPlot &lt;- renderPlot({ # ggplot(data, aes(x = x, y = y)) + # geom_point(size = 0) + # geom_abline(intercept = input$intercept, slope = input$slope, color = &quot;red&quot;, size = 1) # }) # } # # update.packages(ask = FALSE) # # # library(shiny) # library(ggplot2) # # ui &lt;- fluidPage( # titlePanel(&quot;Interactive Line Plot&quot;), # sidebarLayout( # sidebarPanel( # sliderInput(&quot;intercept&quot;, &quot;Intercept:&quot;, min = -10, max = 10, value = 0), # sliderInput(&quot;slope&quot;, &quot;Slope:&quot;, min = -10, max = 10, value = 1) # ), # mainPanel( # plotOutput(&quot;scatterPlot&quot;) # ) # ) # ) # # server &lt;- function(input, output) { # output$scatterPlot &lt;- renderPlot({ # ggplot(data, aes(x = x, y = y)) + # geom_point(size = 0) + # geom_abline(intercept = input$intercept, slope = input$slope, color = &quot;red&quot;, size = 1) # }) # } # # shinyApp(ui = ui, server = server) 15.2.1 Simulated data # 1. In order to see the logic behind the $R^2$, first estimate the linear regression of `LeftToRight` on `Age`. Generate the variable `predicted_LeftToRight` with the generated parameters using [`predict()`](https://www.statology.org/r-lm-predict/) from the point 3 of the previous section (i.e., the one using real data).Generate the graph `predicted_LeftToRight` vs `Age`. # Report the regression here. # ```{r} # result &lt;- lm(LeftToRight ~ age, data = selects19) # selects19$predicted_LeftToRight &lt;- predict(result) # # coeffs1 &lt;- coef(result) # rsq_sd1 &lt;- summary(result)$r.squared # cat(&#39;The R2 is: &#39;, rsq_sd1, &#39;\\n\\nIntercept:&#39;,unname(coeffs1[1]), &#39;\\n\\nSlope:&#39;, unname(coeffs1[2])) # ``` # # Report the graph here. # ```{r} # # Calculate means for Y and X # mean_LeftRight &lt;- mean(selects19$LeftToRight, na.rm = TRUE) # mean_age &lt;- mean(selects19$age, na.rm = TRUE) # # # Create a scatter plot # ggplot(selects19, aes(x = age, y = predicted_LeftToRight)) + # geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;red&quot;) + # Adding the regression line # geom_hline(yintercept = mean_LeftRight, linetype = &quot;dashed&quot;, color = &quot;green&quot;) + # Horizontal line at the mean of y # geom_vline(xintercept = mean_age, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + # Vertical line at the mean of x # scale_y_continuous( # limits = c(0, 10), # breaks = seq(0, 10, by = 1) # ) + # theme_minimal() + # theme( # plot.title = element_text(hjust = 0.5), # Center the plot title # panel.grid.major.y = element_line(color = &quot;grey80&quot;, size = 0.5), # Style for major Y grid lines # panel.grid.minor.y = element_blank() # Remove minor Y grid lines # ) + # labs( # x = &quot;Age (years)&quot;, # y = &quot;Political Preferences (left to right)&quot;, # title = &quot;Scatter plot Age vs. Political Preferences (determ. sim.)&quot; # ) # # # ``` # # 2. Now generate the normally distributed residuals with mean zero and the same variance as the residuals of the model from the previous point. # ```{r} # # Calculate the variance of the residuals # selects19$residual &lt;- resid(result) # residual_sd &lt;- sqrt(var(resid(result))) # # # Set the seed for reproducibility # set.seed(0) # selects19$NormResiduals1 &lt;- rnorm(nrow(selects19), mean = 0, sd = residual_sd) # # ``` # # 3. Simulate new data using both the deterministic (i.e., the the parameters estimated immediately above) and stochastic component (i.e., the residuals form the previous point). Graph the simulated data. # ```{r} # # selects19$Sim_LeftToRight_Det_Stoch &lt;- selects19$predicted_LeftToRight + selects19$NormResiduals1 # # #Means for Y and X # mean_LeftRight &lt;- mean(selects19$Sim_LeftToRight_Det_Stoch, na.rm = TRUE) # mean_age &lt;- mean(selects19$age, na.rm = TRUE) # # # Create a scatter plot # ggplot(selects19, aes(x = age, y = Sim_LeftToRight_Det_Stoch)) + # geom_point(size = 0) + # geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;red&quot;) + # Adding the regression line # geom_hline(yintercept = mean_LeftRight, linetype = &quot;dashed&quot;, color = &quot;green&quot;) + # Horizontal line at the mean of y # geom_vline(xintercept = mean_age, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + # Vertical line at the mean of x # theme_minimal() + # theme(plot.title = element_text(hjust = 0.5)) + # scale_y_continuous( # limits = c(0, 10), # breaks = seq(0, 10, by = 1)) + # theme_minimal() + # theme( # plot.title = element_text(hjust = 0.5), # Center the plot title # panel.grid.major.y = element_line(color = &quot;grey80&quot;, size = 0.5), # Style for major Y grid lines # panel.grid.minor.y = element_blank() # Remove minor Y grid lines # ) + # labs( # x = &quot;Age (years)&quot;, # y = &quot;Simulation: Political Preferences (left to right)&quot;, # title = &quot;Scatter plot Age vs. Political Preferences \\n(determ.&amp; stoch. sim.)&quot; # ) # # ``` # # 4. Now, estimate the regression between the real `age` and simulated `Sim_LeftToRight_Det_Stoch`. Once you do that, generate the $R^2$ for the model. # # ```{r} # result &lt;- lm(LeftToRight ~ age, data = selects19) # rsq_sd1 &lt;- summary(result)$r.squared # cat(&#39;The R2 is: &#39;, rsq_sd1) # # ``` # # 5. Repeat the previous three points but use half the standard deviation of the model of point 1. Is the $R^2$ lower now? Why? See [this](https://en.wikipedia.org/wiki/Coefficient_of_determination) entry to understand the intuition: pay special attention to the graph with the red and blue squares. # # Generate the residuals here. # ```{r} # # Set the seed for reproducibility # set.seed(0) # selects19$NormResiduals10 &lt;- rnorm(nrow(selects19), mean = 0, sd = residual_sd*0.1) # # ``` # # Simulate new data and graph it here. # ```{r} # selects19$Sim_LeftToRight_Det_Stoch_sd10 &lt;- selects19$predicted_LeftToRight + selects19$NormResiduals10 # # #Means for Y and X # mean_LeftRight &lt;- mean(selects19$Sim_LeftToRight_Det_Stoch_sd10, na.rm = TRUE) # mean_age &lt;- mean(selects19$age, na.rm = TRUE) # # # Create a scatter plot # ggplot(selects19, aes(x = age, y = Sim_LeftToRight_Det_Stoch_sd10)) + # geom_point(size = 0) + # geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;red&quot;) + # Adding the regression line # geom_hline(yintercept = mean_LeftRight, linetype = &quot;dashed&quot;, color = &quot;green&quot;) + # Horizontal line at the mean of y # geom_vline(xintercept = mean_age, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + # Vertical line at the mean of x # scale_y_continuous( # limits = c(0, 10), # breaks = seq(0, 10, by = 1)) + # theme_minimal() + # theme(plot.title = element_text(hjust = 0.5)) + # labs( # x = &quot;Age (years)&quot;, # y = &quot;Simulated data (half sd): Political Preferences (left to right)&quot;, # title = &quot;Scatter plot Age vs. Political Preferences&quot; # ) # # ``` # # # Report the new $R^2$ here: # ```{r} # result &lt;- lm(Sim_LeftToRight_Det_Stoch_sd10 ~ age, data = selects19) # rsq_sd10 &lt;- summary(result)$r.squared # cat(&#39;The R2 is: &#39;, rsq_sd10) # # ``` # # 6. Repeat the previous point but use a hundredth the standard deviation of the model of point 1. Is the $R^2$ lower now? Why? # # Generate the residuals here. # ```{r} # # Set the seed for reproducibility # set.seed(0) # selects19$NormResiduals100 &lt;- rnorm(nrow(selects19), mean = 0, sd = residual_sd*0.01) # # ``` # # Simulate new data and graph it here. # ```{r} # selects19$Sim_LeftToRight_Det_Stoch_hsd &lt;- selects19$predicted_LeftToRight + selects19$NormResiduals100 # # #Means for Y and X # mean_LeftRight &lt;- mean(selects19$Sim_LeftToRight_Det_Stoch_hsd, na.rm = TRUE) # mean_age &lt;- mean(selects19$age, na.rm = TRUE) # # # Create a scatter plot # ggplot(selects19, aes(x = age, y = Sim_LeftToRight_Det_Stoch_hsd)) + # geom_point(size = 0) + # geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;red&quot;) + # Adding the regression line # geom_hline(yintercept = mean_LeftRight, linetype = &quot;dashed&quot;, color = &quot;green&quot;) + # Horizontal line at the mean of y # geom_vline(xintercept = mean_age, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + # Vertical line at the mean of x # scale_y_continuous( # limits = c(0, 10), # breaks = seq(0, 10, by = 1)) + # theme_minimal() + # theme(plot.title = element_text(hjust = 0.5)) + # labs( # x = &quot;Age (years)&quot;, # y = &quot;Simulated data (half sd): Political Preferences (left to right)&quot;, # title = &quot;Scatter plot Age vs. Political Preferences&quot; # ) # # ``` # # # Report the new $R^2$ here: # ```{r} # result &lt;- lm(Sim_LeftToRight_Det_Stoch_hsd ~ age, data = selects19) # rsq_sd100 &lt;- summary(result)$r.squared # cat(&#39;The R2 is: &#39;, rsq_sd100) # # ``` # # 7. Present in a table the $R^2$ for each value of the standard deviation. # ```{r} # # table &lt;- data.frame( # SD = c(1, 0.1, 0.01), # R2 = c(rsq_sd1, rsq_sd10, rsq_sd100) # Replace these with the actual R-squared values # ) # # print(table) # # ``` # 8. Graph $R^2$ agains the standard deviation fraction. Fit a linear regression line. # ```{r} # ggplot(table, aes(x = SD, y = R2)) + # geom_point(size = 0) + # geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;green&quot;)+ # theme_minimal() + # theme(plot.title = element_text(hjust = 0.5)) + # labs( # x = &quot;Fraction of standard deviation&quot;, # y = &quot;R2&quot;, # title = &quot;Relation between R2 and standard deviation of residuals&quot; # ) # ``` # 9. [Optional] Graph $R^2$ against the standard deviation fraction. Fit a regression line with a polynomial of degree 2. The regression line fits the data better. Is the polynomial regression more informative than the linear regression? Why? What lesson does this graph give you in terms of the importance of understanding the theoretical relations between variables beyond what an empirical approach could suggest? What does it teach you regarding the tension between under- and over-fitting data? # # ```{r} # ggplot(table, aes(x = SD, y = R2)) + # geom_point() + # geom_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, 2), se = FALSE, color=&#39;green&#39;) + # theme_minimal() + # theme(plot.title = element_text(hjust = 0.5)) + # labs( # x = &quot;Fraction of standard deviation&quot;, # y = &quot;R2&quot;, # title = &quot;Relation between R2 and standard deviation of residuals&quot; # ) # # # # ``` # 10. Print the parameters generated in the point 1. Generate the same predictions as in point 1 and 2 but this time multiply the slope&#39;s parameter by 5, while keeping the intercept unchanged. Call the resulting predictions `predicted_LeftToRight_AdjSlope` Generate a graph with the regression line for `predicted_LeftToRight_AdjSlope` vs `Age` and compare it with the graph `predicted_LeftToRight` vs `Age` generated in point 1. # # Print here # ```{r} # result &lt;- lm(LeftToRight ~ age, data = selects19) # # summary(result) # coeffs &lt;- coef(result) # cat(&#39;Intercept:&#39;,unname(coeffs[1]), &#39;\\n\\nSlope:&#39;, unname(coeffs[2])) # ``` # # Predict with the adjusted slope here. # ```{r} # selects19$predicted_LeftToRight_AdjSlope &lt;- unname(coeffs[1])+(unname(coeffs[2])*5)*selects19$age # ``` # # Generate the graph here. # ```{r} # #Means for Y and X # mean_LeftRight &lt;- mean(selects19$predicted_LeftToRight_AdjSlope, na.rm = TRUE) # mean_age &lt;- mean(selects19$age, na.rm = TRUE) # # # Create a scatter plot # ggplot(selects19, aes(x = age, y = predicted_LeftToRight_AdjSlope)) + # geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;red&quot;) + # Adding the regression line # geom_hline(yintercept = mean_LeftRight, linetype = &quot;dashed&quot;, color = &quot;green&quot;) + # Horizontal line at the mean of y # geom_vline(xintercept = mean_age, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + # Vertical line at the mean of x # theme_minimal() + # theme(plot.title = element_text(hjust = 0.5)) + # scale_y_continuous( # limits = c(0, 15), # breaks = seq(0, 20, by = 1)) + # theme_minimal() + # theme( # plot.title = element_text(hjust = 0.5), # Center the plot title # panel.grid.major.y = element_line(color = &quot;grey80&quot;, size = 0.5), # Style for major Y grid lines # panel.grid.minor.y = element_blank() # Remove minor Y grid lines # ) + # labs( # x = &quot;Age (years)&quot;, # y = &quot;Political Preferences (left to right)&quot;, # title = &quot;Scatter plot Age vs. Political Preferences (adj. slope)&quot; # ) # ``` # 11. Add to the deterministic simulation the same stochastic component generated in point 2. Call that variable `predicted_LeftToRight_AdjSlope_Stoch`. Generate the graph `predicted_LeftToRight_AdjSlope_Stoch` vs `age` with a linear regression line. Generate the $R^2$ and compare it to the one generated in point 4. Why is the new $R^2$ higher than the one in point 4? # # Report the simulation and the graph here. # ```{r} # selects19$predicted_LeftToRight_AdjSlope_Stoch &lt;- selects19$predicted_LeftToRight_AdjSlope + selects19$NormResiduals1 # # #Means for Y and X # mean_LeftRight &lt;- mean(selects19$predicted_LeftToRight_AdjSlope_Stoch, na.rm = TRUE) # mean_age &lt;- mean(selects19$age, na.rm = TRUE) # # # Create a scatter plot # ggplot(selects19, aes(x = age, y = predicted_LeftToRight_AdjSlope_Stoch)) + # geom_point(size = 0) + # geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;red&quot;) + # Adding the regression line # geom_hline(yintercept = mean_LeftRight, linetype = &quot;dashed&quot;, color = &quot;green&quot;) + # Horizontal line at the mean of y # geom_vline(xintercept = mean_age, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + # Vertical line at the mean of x # scale_y_continuous( # limits = c(0, 20), # breaks = seq(0, 20, by = 1)) + # theme_minimal() + # theme( # plot.title = element_text(hjust = 0.5), # Center the plot title # panel.grid.major.y = element_line(color = &quot;grey80&quot;, size = 0.5), # Style for major Y grid lines # panel.grid.minor.y = element_blank() # Remove minor Y grid lines # ) + # labs( # x = &quot;Age (years)&quot;, # y = &quot;Simulation: Political Preferences (left to right)&quot;, # title = &quot;Scatter plot Age vs. Political Preferences \\n(determ.&amp; stoch. sim. with adjusted slope)&quot; # ) # ``` # # Report the new $R^2$ here: # ```{r} # result &lt;- lm(predicted_LeftToRight_AdjSlope_Stoch ~ age, data = selects19) # rsq_sd1_AdjSlope &lt;- summary(result)$r.squared # cat(&#39;The R2 is: &#39;, rsq_sd1_AdjSlope) # # ``` # # Report why is the new $R^2$ higher than the one in point 4? # # ```{r} # print(&#39;The reason the R2 increased is that, while the distance from each observation to the regression line remained unchainged (i.e., the residual was kept unchanged), by increasing the slope of the generating process, the average distance between each observation and the average observation increased (i.e., Y\\&#39;s variance increased). See the graph with the red and blue squares mentioned in point 5.&#39;) # ``` # # 333333333333 # # # 12. Print the parameters generated in the point 1. Generate the same predictions as in point 1 and 2 but this time increase in two units the *intercept&#39;s* parameter (not the *slope&#39;s* parameter as in the point 10), while keeping the slope unchanged. Call the resulting predictions `predicted_LeftToRight_AdjSlope` Generate a graph with the regression line for `predicted_LeftToRight_AdjSlope` vs `Age` and compare it with the graph `predicted_LeftToRight` vs `Age` generated in point 1. # # Print here # ```{r} # result &lt;- lm(predicted_LeftToRight_AdjSlope_Stoch ~ age, data = selects19) # # summary(result) # coeffs12 &lt;- coef(result) # cat(&#39;Intercept:&#39;,unname(coeffs12[1]), &#39;\\n\\nSlope:&#39;, unname(coeffs12[2])) # ``` # # Predict with the adjusted slope here. # ```{r} # selects19$predicted_LeftToRight_AdjIntecept &lt;- unname(coeffs[1]+2)+(unname(coeffs[2]))*selects19$age # ``` # # Generate the graph here. # ```{r} # #Means for Y and X # mean_LeftRight &lt;- mean(selects19$predicted_LeftToRight_AdjIntecept, na.rm = TRUE) # mean_age &lt;- mean(selects19$age, na.rm = TRUE) # # # Create a scatter plot # ggplot(selects19, aes(x = age, y = predicted_LeftToRight_AdjIntecept)) + # geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;red&quot;) + # Adding the regression line # geom_hline(yintercept = mean_LeftRight, linetype = &quot;dashed&quot;, color = &quot;green&quot;) + # Horizontal line at the mean of y # geom_vline(xintercept = mean_age, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + # Vertical line at the mean of x # theme_minimal() + # theme(plot.title = element_text(hjust = 0.5)) + # scale_y_continuous( # limits = c(0, 15), # breaks = seq(0, 20, by = 1)) + # theme_minimal() + # theme( # plot.title = element_text(hjust = 0.5), # Center the plot title # panel.grid.major.y = element_line(color = &quot;grey80&quot;, size = 0.5), # Style for major Y grid lines # panel.grid.minor.y = element_blank() # Remove minor Y grid lines # ) + # labs( # x = &quot;Age (years)&quot;, # y = &quot;Political Preferences (left to right)&quot;, # title = &quot;Scatter plot Age vs. Political Preferences (adj. slope)&quot; # ) # ``` # 13. Add to the deterministic simulation the same stochastic component generated in point 2. Call that variable `predicted_LeftToRight_AdjIntercep_Stoch`. Generate the graph `predicted_LeftToRight_AdjIntecept` vs `age` with a linear regression line. Generate the $R^2$ and compare it to the one generated in point 4. Why is the new $R^2$ higher than the one in point 4? # # Report the simulation and the graph here. # ```{r} # selects19$predicted_LeftToRight_AdjIntercept_Stoch &lt;- selects19$predicted_LeftToRight_AdjIntecept + selects19$residual # # #Means for Y and X # mean_LeftRight &lt;- mean(selects19$predicted_LeftToRight_AdjIntercept_Stoch, na.rm = TRUE) # mean_age &lt;- mean(selects19$age, na.rm = TRUE) # # # Create a scatter plot # ggplot(selects19, aes(x = age, y = predicted_LeftToRight_AdjIntercept_Stoch)) + # geom_point(size = 0) + # geom_smooth(method = &quot;lm&quot;, formula = y ~ x, se = FALSE, color = &quot;red&quot;) + # Adding the regression line # geom_hline(yintercept = mean_LeftRight, linetype = &quot;dashed&quot;, color = &quot;green&quot;) + # Horizontal line at the mean of y # geom_vline(xintercept = mean_age, linetype = &quot;dashed&quot;, color = &quot;orange&quot;) + # Vertical line at the mean of x # scale_y_continuous( # limits = c(0, 20), # breaks = seq(0, 20, by = 1)) + # theme_minimal() + # theme( # plot.title = element_text(hjust = 0.5), # Center the plot title # panel.grid.major.y = element_line(color = &quot;grey80&quot;, size = 0.5), # Style for major Y grid lines # panel.grid.minor.y = element_blank() # Remove minor Y grid lines # ) + # labs( # x = &quot;Age (years)&quot;, # y = &quot;Simulation: Political Preferences (left to right)&quot;, # title = &quot;Scatter plot Age vs. Political Preferences \\n(determ.&amp; stoch. sim. with adjusted intercept)&quot; # ) # ``` # Regress the `predicted_LeftToRight_AdjIntercept_Stoch` on `age` # ```{r} # result &lt;- lm(predicted_LeftToRight_AdjIntercept_Stoch ~ age, data = selects19) # rsq_sd1_AdjIntercept &lt;- summary(result)$r.squared # coeffs13 &lt;- coef(result) # cat(&#39;Intercept:&#39;,unname(coeffs13[1]), &#39;\\n\\nSlope:&#39;, unname(coeffs13[2])) # ``` # # Report the $R^2$ generated in points 2, 11, and 13 here: # ```{r} # tableSD1 &lt;- data.frame( # LookR2 = c(&#39;Point 2&#39;, &#39;Point 12&#39;, &#39;Point 13&#39;), # Model = c(&#39;Unchanged&#39;, &#39;P2 + (slope*5)&#39;, &#39;P2+(intercept+2)&#39;), # R2 = c(rsq_sd1, rsq_sd1_AdjSlope, rsq_sd1_AdjIntercept), # LookParams = c(&#39;Point 1&#39;, &#39;Point 12&#39;, &#39;Point 13&#39;), # Intercept = c(unname(coeffs1[1]), unname(coeffs12[1]), unname(coeffs13[1])), # Slope = c(unname(coeffs1[2]), unname(coeffs12[2]), unname(coeffs13[2])) # ) # # # print(tableSD1) # # ``` # # 14. Comparing the $R^2$ and the parameters across the models of points 1, 2, and 3 answer the following questions: # # i. What effect does increasing the slope of the data generating process (while keeping the residuals and intercept unchanged) have on the $R^2$? # ```{r} # print(&#39;The R2 increases as seen in point 11&#39;) # ``` # # ii. What effect does increasing the intercept of the data generating process (while keeping the residuals and slope unchanged) have on the $R^2$? # ```{r} # print(&#39;The R2 stays unchanged as the distance from each observation to the regression line remained unchainged (i.e., the residual was kept unchanged) as well as the average distance between each observation and the average observation remained unchainged (i.e., Y\\&#39;s variance was kept unchanged)&#39;) # # ``` # # iii. What effect does increasing the slope of the data generating process (while keeping the residuals and intercept unchanged) have on the intercept and slope? # ```{r} # print(&#39;It increases the estimated slope and decreases the intercept.&#39;) # ``` # # iv. What effect does increasing the intercept of the data generating process (while keeping the residuals and slope unchanged) have on the intercept and slope? # ```{r} # print(&#39;It keeps the estimated slope unchanged and the estimated intercept is the same as before the adjustment but increases by the same value that the whole data generating process was modified.&#39;) "],["data-transformation.html", "Chapter 16 Data transformation 16.1 Week 1: Gorilla 16.2 Week 4: restricted variables from SELECTS", " Chapter 16 Data transformation 16.1 Week 1: Gorilla The original data of the original paper was adjusted to put it in a substantively relevant format. female &lt;- read.table(&quot;/Users/fperil/Documents/0_IPZ/2023_2/Leemann-QuantMethods/QuantitativeMethods/OriginalData/f.txt&quot;, header = TRUE, sep = &quot;\\t&quot;) male &lt;- read.table(&quot;/Users/fperil/Documents/0_IPZ/2023_2/Leemann-QuantMethods/QuantitativeMethods/OriginalData/m.txt&quot;, header = TRUE, sep = &quot;\\t&quot;) male &lt;- dplyr::rename(male, ScreenTime = steps, ClosenessPopulistParty=bmi ) female &lt;- dplyr::rename(female, ScreenTime = steps, ClosenessPopulistParty=bmi ) female &lt;-female %&gt;% mutate(ClosenessPopulistParty= 10*(female$ClosenessPopulistParty-min(female$ClosenessPopulistParty))/(max(ClosenessPopulistParty)-min(female$ClosenessPopulistParty))) male &lt;-male %&gt;% mutate(ClosenessPopulistParty= 10*(male$ClosenessPopulistParty-min(male$ClosenessPopulistParty))/(max(male$ClosenessPopulistParty)-min(male$ClosenessPopulistParty))) write.table(female, &quot;Data/f.txt&quot;, sep = &quot;;&quot;) write.table(male, &quot;Data/m.txt&quot;, sep = &quot;;&quot;) 16.2 Week 4: restricted variables from SELECTS selects19 &lt;- read.csv(&quot;/Users/fperil/Documents/0_IPZ/2023_2/Leemann-QuantMethods/QuantitativeMethods/OriginalData/SELECTS 2019/1179_Selects2019_PES_Data_v1.1.0.csv&quot;, header = TRUE) selects19 &lt;- selects19 %&gt;% select(sex, age, matches(&quot;f15200&quot;), f20221) %&gt;% filter(!is.na(selects19$sex) &amp; !is.na(selects19$age) &amp; !is.na(selects19$f15200) &amp; !is.na(selects19$f20221)) # Remove rows where x or y is NA selects19$f15200 &lt;- as.numeric(selects19$f15200) write.table(selects19, file = &quot;Data/SELECTS 2019/data.csv&quot;, sep = &quot;,&quot;, row.names = FALSE) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
